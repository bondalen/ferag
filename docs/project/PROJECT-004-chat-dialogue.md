# Диалог по RAG (чат): архитектура и порядок работы

Приложение к [PROJECT-004](PROJECT-004.md) (веб-приложение и развёртывание). Описание текущего состояния чата по RAG, целей полноценного диалога, вариантов накопления контекста и сжатия длинных диалогов.

---

## 1. Текущее состояние

| Компонент | Как работает |
|-----------|---------------|
| **Backend** `POST /rags/{id}/chat` | Принимает один `question`. Берёт контекст из Fuseki (`build_context_by_question`), формирует один user-message с инструкцией + контекстом + вопросом, вызывает LLM, возвращает ответ. История не пишется в БД. |
| **Frontend** `ChatView.vue` | Сообщения хранятся в `ref`; после перезагрузки страницы диалог пустой. |
| **rag_llm** | `answer_from_context(context, question)` — один вызов с одним user message. Нет формата «диалог из нескольких пар вопрос–ответ». |

---

## 2. Цели полноценного диалога

- **Для пользователя:** видеть историю реплик (в т.ч. после перезагрузки), задавать уточняющие вопросы («а что насчёт X?», «расскажи подробнее»).
- **Для модели:** при ответе на N-й вопрос учитывать предыдущие реплики (контекст диалога), чтобы ответы были связными.

---

## 3. Накопление контекста

### 3.1. Хранение истории (обязательный минимум)

- **Где:** таблица в БД, например `chat_messages (id, rag_id, user_id, role, content, context_used, created_at)`. `role`: `user` | `assistant`.
- **Зачем:** показывать историю на фронте, подгружать при открытии страницы диалога; при желании — передавать последние K пар в LLM.
- **API:**  
  - `POST /rags/{id}/chat` — как сейчас, плюс запись двух строк в `chat_messages` (user + assistant).  
  - `GET /rags/{id}/chat/messages` — последние N сообщений (или по session_id, если позже введёте сессии).

После этого диалог сохраняется и отображается (полноценный чат с точки зрения UX).

### 3.2. Контекст для LLM при каждом запросе

Сейчас в модель уходит один блок: инструкция + контекст из графа + текущий вопрос. Чтобы модель «помнила» диалог, в запрос нужно добавлять предыдущие пары вопрос–ответ. Варианты:

| Вариант | Описание | Плюсы | Минусы |
|---------|----------|--------|--------|
| **A. Без истории** | Как сейчас: только текущий вопрос + RAG-контекст. | Просто, мало токенов. | Нет связности с предыдущими репликами. |
| **B. Скользящее окно** | В запрос к LLM передаём последние K пар (например 3–5) «user / assistant» + текущий вопрос + RAG-контекст. | Модель понимает уточнения и продолжения. | Рост числа токенов, нужен лимит. |
| **C. Окно по токенам** | То же, но «последние N токенов диалога» (остальное обрезаем). | Контроль размера контекста. | Сложнее (токенизация или лимит символов). |
| **D. Резюме диалога** | Периодически вызывать LLM: «кратко резюмируй диалог»; в следующий запрос кладём резюме + последняя пара + текущий вопрос. | Длинный диалог не раздувает промпт. | Доп. вызов LLM, резюме может терять детали. |

**Практичная отправная точка:** вариант **B (скользящее окно)** с K = 3–5 пар. При `POST /rags/{id}/chat` читать из БД последние 2K сообщений, формировать `messages=[system, (user, assistant)*K, user (текущий вопрос с RAG-контекстом)]`, вызывать LLM.

### 3.3. Где формировать диалог для LLM

- **В backend:** при обработке `POST /rags/{id}/chat` запрашивать из БД последние сообщения, собирать список `messages` и вызывать `rag_llm` с новой сигнатурой, например `answer_from_context_with_history(rag_context, messages_list)`.
- **В rag_llm:** добавить функцию, принимающую список сообщений и RAG-контекст (в system или в последний user). OpenAI-совместимый API поддерживает `messages=[{role, content}, ...]`.

---

## 4. Порядок действий (реализация)

1. **История в БД и на экране (без изменения логики LLM)**  
   Миграция: таблица `chat_messages`. Backend: после ответа LLM писать в БД две записи (user, assistant). Добавить `GET /rags/{id}/chat/messages`. Frontend: при открытии диалога запрашивать историю и заполнять `messages`. Итог: диалог сохраняется и отображается.

2. **Передача последних K пар в LLM (накопление контекста)**  
   Backend: при `POST /rags/{id}/chat` загружать последние 2K записей из `chat_messages`, собирать `messages` для API. В `rag_llm`: вызов с `messages=[system, ...pairs, current_user_message]`. Итог: модель «видит» последние реплики.

3. **По желанию позже:** лимит по токенам/символам; сессии диалога (session_id); резюме при длинных чатах (см. ниже).

---

## 5. Длинные диалоги: сжатие и индексация

Когда «последние K пар» не помещаются в контекстное окно или нужно учитывать старые реплики, используют **сжатие** и/или **выборочную подстановку**.

### 5.1. Подходы в промышленности

| Подход | Суть | Где встречается |
|--------|------|------------------|
| **Суммаризация (резюме)** | Старую часть диалога прогоняют через LLM: «кратко резюмируй ключевые факты и решения». В запрос: **резюме + последние M пар + текущий вопрос**. | LangChain ConversationSummaryBufferMemory, Honcho, многие чат-боты. |
| **Скользящее окно + резюме** | Резюме «давнего» прошлого + последние реплики в полном виде. | Honcho (резюме каждые 20/60 сообщений), Pipecat. |
| **Выборочная подстановка** | Индексировать сообщения (эмбеддинги); в запрос подставлять только релевантные текущему вопросу реплики. | RAG по истории чата; Cursor — dynamic context discovery. |
| **Периодическое сжатие** | При заполнении контекста вызывать сжатие: модель возвращает укороченную версию контекста; сжатое хранится, свежие реплики добавляются как есть. | Cursor: суммаризация при заполнении; доступ к полной истории при необходимости. |

### 5.2. Что сделать в ferag

- **Резюме + скользящее окно:** хранить все сообщения в БД. Периодически (каждые 10–20 пар или при превышении лимита токенов) вызывать LLM для резюме старых сообщений, сохранять в `chat_session_summary` (или аналог). При каждом `POST /rags/{id}/chat`: **последнее резюме + последние K пар + текущий вопрос + RAG-контекст**.
- **Опционально позже:** индексация истории (эмбеддинги, pgvector) для выборочной подстановки релевантных реплик; гибрид (резюме + окно + 1–2 релевантные пары из середины).

Итог: передавать «весь» объём чата модели можно через **сжатие (резюме)** и/или выборочную подстановку; в запрос передаётся ограниченный по токенам вариант: резюме + последние K пар + текущий вопрос + RAG.

---

## 6. Кратко

- **Накопление контекста:** (1) хранить историю в БД, (2) передавать в LLM последние K пар (скользящее окно). Сначала реализовать сохранение и показ истории, затем окно.
- **Длинные диалоги:** периодическая суммаризация старых сообщений; в запрос: резюме + последние K пар + текущий вопрос + RAG. При необходимости — индексация истории (RAG по чату).

Задачи в [TASKS.md](../tasks/TASKS.md) (Блок 9: 9.11, 9.12, 9.13).
