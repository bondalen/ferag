# Чат 2 — Worker + полный цикл обновления RAG

**Дата:** 2026-02-19 (план подготовлен в конце Чата 1)  
**Предшественник:** [26-0218-1703_plan.md](26-0218-1703_plan.md) (backend skeleton, все критерии Чата 1 выполнены).

---

## Исходное состояние

- **Backend**: FastAPI работает; auth (`/auth/register`, `/auth/login`, `/auth/me`), rags (`/rags`), tasks (`/tasks/{id}`, `/rags/{id}/tasks`) протестированы.
- **БД**: 5 таблиц (users, rag_instances, rag_members, upload_cycles, tasks); Alembic migration применена.
- **Fuseki**: запущен на `localhost:43030`; `create_dataset` / `delete_dataset` работают.
- **code/worker/**: не реализован; Dockerfile не существует; `docker compose up worker` упадёт.
- **graphrag-test/**: рабочий pipeline в виде standalone-скриптов (`test_graphrag_to_rdf.py`, `test_schema_induction.py`, `merge_ontologies.py`, `merge_triples.py`); все функции завёрнуты в `main()` — не импортируемы напрямую.
- **Redis**: отсутствует на nb-win; нужен для Celery broker и WebSocket pub/sub.

---

## Задача плана

1. Добавить Redis в `deploy/nb-win/docker-compose.yml`.
2. Реализовать `code/worker/` — Celery-приложение с цепочкой задач.
3. Адаптировать скрипты `graphrag-test/` для программного вызова (`graphrag_lib`).
4. Расширить backend: `POST /rags/{id}/upload`, WebSocket `/ws/tasks/{id}`, `POST /rags/{id}/cycles/{id}/approve`.
5. Новый роутер `/rags/{id}/chat` — RAG-вопрос к prod-датасету через LLM.
6. Финальная проверка: upload → WebSocket → approve → chat.

---

## Исходное состояние (файлы и сервисы)

```
ferag/
├── code/
│   ├── backend/           # Готово (Чат 1)
│   └── worker/            # ❌ Не существует
├── graphrag-test/         # Скрипты pipeline (нужна адаптация)
└── deploy/
    └── nb-win/
        └── docker-compose.yml   # Нужно добавить redis
```

**Сервисы перед началом:**

| Сервис      | Статус         | Порт           |
|-------------|----------------|----------------|
| PostgreSQL  | ✅ запущен      | localhost:45432 |
| Fuseki      | ✅ запущен      | localhost:43030 |
| Redis       | ✅ запущен (cr-ubu, ferag-redis) | 10.7.0.1:47379 |
| LM Studio   | ⚡ нужен для LLM | Windows:41234  |
| WireGuard   | ✅ активен      | 10.7.0.1 ↔ 10.7.0.3 |

---

## Соглашения

### Рабочие каталоги задач
Каждый цикл обновления работает в изолированном каталоге:
```
/tmp/ferag/rag_{rag_id}/cycle_{cycle_id}/
├── input/
│   └── source.txt          # Загруженный файл
├── output/                 # Результаты GraphRAG (parquet)
├── graphrag_output.ttl     # RDF из GraphRAG
├── extracted_ontology.ttl  # Schema Induction
├── integrated_ontology.ttl # После merge_ontologies
└── integrated_triples.ttl  # После merge_triples
```

### Redis channels
- `task:{task_id}` — pub/sub для WebSocket статусов
- Сообщения: `{"status": "running|done|failed", "step": "graphrag|schema|merge_onto|merge_triples|staging", "error": null}`

### Именование Fuseki датасетов (уже в fuseki_admin.py)
| Функция                          | Датасет              |
|----------------------------------|----------------------|
| `rag_prod_dataset(rag_id)`       | `ferag-{id:05d}`     |
| `rag_staging_dataset(rag_id)`    | `ferag-{id:05d}-stg` |
| `rag_triples_dataset(rag_id)`    | `ferag-{id:05d}-tri` |
| `rag_ontology_dataset(rag_id)`   | `ferag-{id:05d}-ont` |

### Celery broker/backend
```
# nb-win worker (deploy/nb-win/docker-compose.yml):
CELERY_BROKER_URL=redis://10.7.0.1:47379/0      # Redis на cr-ubu через WireGuard
CELERY_RESULT_BACKEND=redis://10.7.0.1:47379/1

# cr-ubu backend (deploy/cr-ubu/docker-compose.yml, внутри Docker-сети):
REDIS_URL=redis://redis:47379/0
```

---

## Шаги

### ✅ 1. Среда: Redis + структура worker

#### ✅ 1.1 Развернуть Redis на cr-ubu

Redis развёртывается на cr-ubu как **отдельный контейнер** — это финальная продуктивная конфигурация (без supervisord). Файл `deploy/cr-ubu/docker-compose.yml` уже обновлён: сервис `redis` добавлен, том `redis-data` прописан.

```bash
# На cr-ubu (SSH или консоль), из корня репозитория (например /opt/ferag):
./deploy/cr-ubu/up-redis.sh
# или: cd deploy/cr-ubu && docker compose up -d redis
```

Проверка с nb-win (WireGuard должен быть активен):
```bash
redis-cli -h 10.7.0.1 -p 47379 ping   # → PONG
```

nb-win worker уже настроен на `CELERY_BROKER_URL=redis://10.7.0.1:47379/0` — изменений не требует.

**Исполнение:** на cr-ubu установлен `docker-compose` v1.29.2 (без плагина Compose v2); проект на cr-ubu отсутствует, поэтому `docker-compose up` не применялся. Контейнер поднят напрямую через `docker run` с теми же параметрами (image `redis:7-alpine`, порт `10.7.0.1:47379:47379`, том `ferag-redis-data`, `--restart unless-stopped`). Контейнер `ferag-redis` запущен (`Up`). Проверка: `docker exec ferag-redis redis-cli -p 47379 ping` → `PONG`.

#### ✅ 1.2 Создать структуру `code/worker/`

```
code/worker/
├── __init__.py            # Пакет worker (для celery -A worker.celery_app)
├── Dockerfile
├── requirements.txt
├── celery_app.py
├── config.py
├── fuseki_client.py       # Обёртка над SPARQL/Fuseki (из backend)
└── tasks/
    ├── __init__.py
    ├── base.py            # get_db_session(), publish_status(), update_task()
    ├── graphrag_task.py
    ├── schema_task.py
    ├── merge_task.py
    └── staging_task.py
```

**Исполнение:** созданы каталог `code/worker/` и все файлы по плану; добавлен `code/worker/__init__.py` для пакета. `config.py` — BaseSettings (celery_broker_url, celery_result_backend, database_url, fuseki_*, llm_api_url, llm_model, work_dir). `fuseki_client.py` — копия логики backend `fuseki_admin.py` с импортом `worker.config`. `tasks/base.py` — get_db_session(), publish_status(redis, task_id, status, step, error), update_task(db, task_id, status, error). Задачи — заглушки с `@celery.task(bind=True)` и TODO под шаги 3.2–3.5. В `deploy/nb-win/docker-compose.yml` для worker добавлены CELERY_RESULT_BACKEND, FUSEKI_USER, FUSEKI_PASSWORD. Проверка: `docker compose build worker` — успешно.

#### ✅ 1.3 `code/worker/requirements.txt`

```
celery[redis]
redis
sqlalchemy
psycopg2-binary
httpx
graphrag          # MS GraphRAG
llama-index-core
llama-index-llms-openai-like
rdflib
python-dotenv
pydantic-settings
```

**Исполнение:** файл создан в шаге 1.2; содержимое приведено в соответствие с планом (добавлен комментарий к graphrag). Сборка образа worker с этими зависимостями успешна.

#### ✅ 1.4 `code/worker/config.py`

Аналогично backend: `pydantic_settings.BaseSettings`, загрузка из `.env`:
- `celery_broker_url`, `celery_result_backend`
- `database_url`
- `fuseki_url`, `fuseki_user`, `fuseki_password`
- `llm_api_url` (LM Studio или OpenAI-совместимый)
- `llm_model` (имя модели, e.g. `lmstudio-community/Meta-Llama-3.3-70B-Instruct-UDLQ4_K_M`)
- `work_dir` = `/tmp/ferag` (базовый каталог рабочих файлов)

**Исполнение:** файл создан в шаге 1.2; все поля по плану присутствуют (BaseSettings, model_config с .env, get_settings() с @lru_cache). Добавлены комментарии к группам полей. Значения по умолчанию: llm_api_url (LM Studio в Docker), llm_model, work_dir.

#### ✅ 1.5 `code/worker/celery_app.py`

```python
from celery import Celery
from worker.config import get_settings  # или config напрямую

settings = get_settings()

celery = Celery(
    "ferag_worker",
    broker=settings.celery_broker_url,
    backend=settings.celery_result_backend,
    include=[
        "tasks.graphrag_task",
        "tasks.schema_task",
        "tasks.merge_task",
        "tasks.staging_task",
    ],
)

celery.conf.update(task_serializer="json", result_serializer="json")
```

**Исполнение:** файл создан в шаге 1.2. Содержимое соответствует плану (get_settings(), Celery с broker/backend из config, include четырёх модулей задач, task_serializer/result_serializer="json"). В include использованы полные имена `worker.tasks.*` для однозначной загрузки при `-A worker.celery_app:celery`.

#### ✅ 1.6 `code/worker/Dockerfile`

```dockerfile
FROM python:3.11-slim

RUN apt-get update && apt-get install -y \
    build-essential curl \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . /app/worker

# graphrag-test монтируется как том (см. docker-compose.yml)
ENV PYTHONPATH=/app:/app/graphrag-test

CMD ["celery", "-A", "worker.celery_app:celery", "worker", "--loglevel=info", "--concurrency=2"]
```

Проверка: `docker compose build worker` без ошибок.

**Исполнение:** Dockerfile создан в шаге 1.2; содержимое совпадает с планом (python:3.11-slim, build-essential curl, WORKDIR /app, COPY requirements + pip install, COPY . /app/worker, PYTHONPATH, CMD celery). Сборка образа в шаге 1.2 выполнена успешно.

---

### ✅ 2. Адаптация `graphrag-test/` как библиотеки

#### ✅ 2.1 Проанализировать скрипты-кандидаты

Каждый скрипт имеет `main()`. Для программного вызова нужно вынести логику в функции с параметрами:

| Скрипт                    | Функция-цель                             | Вход                        | Выход                         |
|---------------------------|------------------------------------------|-----------------------------|-------------------------------|
| `test_graphrag_to_rdf.py` | `graphrag_to_rdf(root_dir) -> Path`      | путь к output/ GraphRAG     | `graphrag_output.ttl`         |
| `test_schema_induction.py`| `run_schema_induction(root_dir, llm_cfg) -> Path` | output/ + LLM cfg | `extracted_ontology.ttl`      |
| `merge_ontologies.py`     | `merge_ontologies(onto1, onto2) -> Path` | 2 TTL файла                 | `integrated_ontology.ttl`     |
| `merge_triples.py`        | `merge_triples(triples1, triples2) -> Path` | 2 TTL файла              | `integrated_triples.ttl`      |

Стратегия: **минимальные правки скриптов** — вынести логику в функции с параметрами; `main()` вызывает их. Создать `graphrag_lib/` с тонкими обёртками, импортирующими эти функции.

#### ✅ 2.2 Создать `graphrag-test/graphrag_lib/__init__.py`

Экспортировать: `run_graphrag_pipeline`, `run_schema_induction`, `merge_ontologies`, `merge_triples`.

Каждая функция:
- Получает рабочий каталог цикла или пути (`work_dir: Path` / пути к TTL)
- Выполняет логику (импорт функций из скриптов)
- Возвращает путь к артефакту
- При ошибке — бросает исключение с понятным сообщением

Проверка: `python -c "from graphrag_lib import run_graphrag_pipeline; print('ok')"` из `graphrag-test/` (требует окружение с pandas, rdflib — есть в образе worker).

**Исполнение:** В скрипты добавлены вызываемые функции без изменения поведения CLI: `test_graphrag_to_rdf.py` — `graphrag_to_rdf(root_dir, output_path)`; `test_schema_induction.py` — `run_schema_induction(root_dir, output_path, llm_base_url, model)`; `merge_ontologies.py` — `merge_ontologies(path1, path2, out_path, report_path=None)`; `merge_triples.py` — `merge_triples(path1, path2, out_path, report_path=None)`. Создан `graphrag-test/graphrag_lib/__init__.py`: добавление `graphrag-test` в `sys.path`, реэкспорт четырёх обёрток с сигнатурами под work_dir/пути. Проверка импорта выполняется в образе worker (PYTHONPATH=/app:/app/graphrag-test).

---

### ✅ 3. Celery задачи

#### ✅ 3.1 `tasks/base.py` — вспомогательные функции

```python
import redis
from sqlalchemy import create_engine
from sqlalchemy.orm import Session

def get_db_session() -> Session: ...          # создаёт сессию по DATABASE_URL
def publish_status(r, task_id, status, step, error=None): ...  # r.publish(f"task:{task_id}", json)
def update_task(db, task_id, status, error=None): ...           # обновляет Task в БД
```

**Исполнение:** реализовано в шаге 1.2. В `base.py`: `get_db_session()` (SessionLocal по DATABASE_URL), `publish_status(r, task_id, status, step, error=None)` — публикация JSON в Redis channel `task:{task_id}`; `update_task(db, task_id, status, error=None)` — UPDATE tasks и commit. Добавлен `get_redis()` — клиент Redis по CELERY_BROKER_URL для использования в задачах при вызове `publish_status`.

#### ✅ 3.2 `tasks/graphrag_task.py` — `run_graphrag`

```python
@celery.task(bind=True)
def run_graphrag(self, rag_id: int, cycle_id: int, task_id: int, input_file: str):
    # 1. Подготовить work_dir/input/source.txt
    # 2. Создать settings.yaml для этого запуска (шаблон из graphrag-test/settings.yaml,
    #    с заменой путей input/output на work_dir)
    # 3. graphrag index --root work_dir
    # 4. graphrag_lib.run_graphrag_pipeline(work_dir) → graphrag_output.ttl
    # 5. update_task + publish_status
```

Задача долгая (5–30 минут). `task_time_limit=3600`.

**Исполнение:** work_dir = config.work_dir / rag_{rag_id} / cycle_{cycle_id}. Подготовка: копирование input_file в work_dir/input/source.txt; запись settings.yaml из шаблона graphrag-test с подстановкой llm_api_url и llm_model; копирование prompts/ в work_dir. Запуск `graphrag index --root work_dir` (subprocess, timeout 3600). Вызов graphrag_lib.run_graphrag_pipeline(work_dir). В начале шага — publish_status(running, graphrag); при успехе — publish_status(done, graphrag); при ошибке — update_task(failed), publish_status(failed), raise. Добавлены time_limit=3600, soft_time_limit=3600. В config добавлен graphrag_test_dir (по умолчанию /app/graphrag-test).

#### ✅ 3.3 `tasks/schema_task.py` — `run_schema_induction`

```python
@celery.task(bind=True)
def run_schema_induction(self, rag_id: int, cycle_id: int, task_id: int):
    # graphrag_lib.run_schema_induction(work_dir, llm_cfg) → extracted_ontology.ttl
    # update + publish
```

**Исполнение:** work_dir = config.work_dir / rag_{rag_id} / cycle_{cycle_id}. Добавление graphrag_test_dir в sys.path, импорт graphrag_lib.run_schema_induction. Вызов с work_dir, settings.llm_api_url, settings.llm_model. В начале — publish_status(running, schema_induction); при успехе — publish_status(done, schema_induction); при ошибке — update_task(failed), publish_status(failed), raise. time_limit=3600, soft_time_limit=3600.

#### ✅ 3.4 `tasks/merge_task.py` — `merge_ontologies`, `merge_triples`

```python
@celery.task(bind=True)
def do_merge(self, rag_id: int, cycle_id: int, task_id: int):
    # Скачать prod-онтологию из Fuseki (-ont dataset) в prod_ontology.ttl
    # graphrag_lib.merge_ontologies(extracted, prod_onto) → integrated_ontology.ttl
    # Скачать prod-триплеты из Fuseki (prod dataset) в prod_triples.ttl
    # graphrag_lib.merge_triples(graphrag_output, prod_triples) → integrated_triples.ttl
    # update + publish
```

**Исполнение:** work_dir = config.work_dir / rag_{rag_id} / cycle_{cycle_id}. В fuseki_client добавлена export_dataset_to_ttl(dataset_name, out_path) — CONSTRUCT { ?s ?p ?o } WHERE { ?s ?p ?o } с Accept: text/turtle; при 404 пишется пустой TTL. do_merge: publish_status(running, merge); экспорт prod через rag_prod_dataset(rag_id) в work_dir/prod_export.ttl; импорт graphrag_lib.merge_ontologies, merge_triples; merge_ontologies(extracted_ontology.ttl, prod_export.ttl) → integrated_ontology.ttl; merge_triples(graphrag_output.ttl, prod_export.ttl) → integrated_triples.ttl; при успехе publish_status(done, merge); при ошибке update_task(failed), publish_status(failed), raise. time_limit=3600, soft_time_limit=3600.

#### ✅ 3.5 `tasks/staging_task.py` — `load_to_staging`

```python
@celery.task(bind=True)
def load_to_staging(self, rag_id: int, cycle_id: int, task_id: int):
    # create_dataset(rag_triples_dataset(rag_id))
    # create_dataset(rag_ontology_dataset(rag_id))
    # create_dataset(rag_staging_dataset(rag_id))
    # SPARQL UPDATE: загрузить integrated_triples.ttl → -tri
    # SPARQL UPDATE: загрузить integrated_ontology.ttl → -ont
    # (staging = виртуальный union; или загрузить в -stg = объединение -tri + -ont)
    # db: UploadCycle.status = 'review'
    # db: Task.status = 'done'
    # publish {"status": "done", "step": "staging"}
```

**Исполнение:** work_dir = config.work_dir / rag_{rag_id} / cycle_{cycle_id}. В fuseki_client добавлена load_ttl_into_dataset(dataset_name, ttl_path) — PUT в /{dataset}/data?default (GSP, default graph), Content-Type: text/turtle. В base: get_cycle_n(db, cycle_id), update_upload_cycle_status(db, cycle_id, status). load_to_staging: publish_status(running, staging); cycle_n = get_cycle_n(db, cycle_id); create_dataset для -triples, -ontology, -staging; load_ttl_into_dataset(-tri, integrated_triples.ttl), load_ttl_into_dataset(-ont, integrated_ontology.ttl); update_upload_cycle_status(db, cycle_id, 'review'); update_task(db, task_id, 'done'); publish_status(done, staging); при ошибке update_task(failed), publish_status(failed), raise. time_limit=3600, soft_time_limit=3600.

#### ✅ 3.6 Цепочка задач — сборка chain

В `tasks/__init__.py` или отдельном модуле:

```python
from celery import chain

def start_update_chain(rag_id, cycle_id, task_id, input_file):
    return chain(
        run_graphrag.s(rag_id, cycle_id, task_id, input_file),
        run_schema_induction.s(rag_id, cycle_id, task_id),
        do_merge.s(rag_id, cycle_id, task_id),
        load_to_staging.s(rag_id, cycle_id, task_id),
    ).apply_async()
```

Ошибка в любом шаге: `on_failure` callback → `Task.status = 'failed'`, publish error.

**Исполнение:** В `worker/tasks/__init__.py`: импорт chain, run_graphrag, run_schema_induction, do_merge, load_to_staging; функция start_update_chain(rag_id, cycle_id, task_id, input_file) возвращает chain( run_graphrag.s(...), run_schema_induction.si(...), do_merge.si(...), load_to_staging.si(...) ).apply_async(link_error=on_chain_failure.s()). Для шагов 2–4 использован .si(), чтобы не передавать результат предыдущего шага. В `worker/tasks/base.py`: задача on_chain_failure(request, excinfo) — из request.args[2] берётся task_id, update_task(db, task_id, 'failed', err_msg), publish_status(r, task_id, 'failed', step='', error=err_msg). В `celery_app.py` в include добавлен "worker.tasks".

---

### ✅ 4. Backend: новые эндпоинты

#### ✅ 4.1 `POST /rags/{rag_id}/upload`

```python
@router.post("/{rag_id}/upload")
async def upload_file(rag_id: int, file: UploadFile = File(...),
                      db: Session = Depends(get_db),
                      current_user: User = Depends(get_current_user)):
    rag = _can_access_rag(db, current_user, rag_id)
    # Сохранить файл в /tmp/ferag/rag_{rag_id}/cycle_{cycle_id}/input/source.txt
    # Создать UploadCycle (status='pending')
    # Создать Task (type='full_cycle', status='running')
    # start_update_chain.delay(rag_id, cycle.id, task.id, file_path)
    return {"cycle_id": cycle.id, "task_id": task.id}
```

Ограничения: только owner может загружать; Content-Type: text/plain или .txt.

**Исполнение:** В `app/config.py` добавлен `work_dir: Path = Path("/tmp/ferag")`. В `app/routers/rags.py`: в начало добавлен путь `code/` в sys.path для импорта worker; эндпоинт `POST /rags/{rag_id}/upload` — проверка доступа и что только owner; проверка Content-Type (text/plain или application/octet-stream); создание UploadCycle(rag_id, cycle_n=rag.cycle_count+1, status='pending'); создание каталога work_dir/rag_{id}/cycle_{cycle.id}/input и сохранение файла как source.txt; создание Task(rag_id, cycle_id, type='full_cycle', status='running'); вызов start_update_chain(rag_id, cycle.id, task.id, file_path); при ошибке запуска — Task.status='failed', 503. Ответ: UploadResponse(cycle_id, task_id).

#### ✅ 4.2 WebSocket `/ws/tasks/{task_id}`

Добавить в `main.py`:
```python
from fastapi import WebSocket
from app.deps import get_current_user_ws   # новая зависимость: token из query param

@app.websocket("/ws/tasks/{task_id}")
async def ws_task_status(websocket: WebSocket, task_id: int, token: str):
    # Аутентифицировать token
    # Проверить доступ к task
    # Подписаться на Redis channel f"task:{task_id}"
    # Стримить сообщения клиенту
    # Закрыть при status == "done" или "failed"
```

Для async Redis подписки: `redis.asyncio` (входит в `redis[asyncio]`).

Добавить в `requirements.txt` backend: `redis[asyncio]` (уже есть `redis` как dep Celery, убедиться в наличии asyncio extra).

**Исполнение:** В `requirements.txt` добавлен `redis[asyncio]`. В `app/config.py` — `redis_url: str = "redis://localhost:6379/0"`. В `app/deps.py`: `get_current_user_ws(token, db)` — декодирование JWT, загрузка User; при ошибке — `WebSocketException(code=1008, reason=...)`. В `main.py`: эндпоинт `GET /ws/tasks/{task_id}?token=...` — accept; проверка JWT и доступа к task через _can_access_rag(db, user, task.rag_id); подписка на Redis channel `task:{task_id}` через `redis.asyncio`, цикл get_message → send_json(payload), выход при status in ("done", "failed"); в finally — unsubscribe, закрытие pubsub и db.

#### ✅ 4.3 `POST /rags/{rag_id}/cycles/{cycle_id}/approve`

```python
@router.post("/{rag_id}/cycles/{cycle_id}/approve")
def approve_cycle(rag_id: int, cycle_id: int, ...):
    # Проверить owner
    # Проверить cycle.status == 'review'
    # SPARQL COPY: скопировать содержимое -stg → в prod dataset (-prod)
    #   (через fuseki_admin: DELETE всех триплетов в prod, INSERT из stg)
    # Удалить датасеты: -tri, -ont, -stg
    # UploadCycle.status = 'merged'
    # RagInstance.cycle_count += 1
    # Вернуть 200 {message: "approved"}
```

**Исполнение:** В `app/fuseki_admin.py` добавлены: `sparql_update(dataset, update_body)` — POST /{dataset}/update; `get_dataset_ttl(dataset)` — CONSTRUCT, возврат TTL-строки; `put_dataset_ttl(dataset, ttl_content)` и `post_dataset_ttl(dataset, ttl_content)` — Graph Store PUT/POST в default graph. Эндпоинт `POST /rags/{rag_id}/cycles/{cycle_id}/approve`: только owner; цикл по cycle_id, cycle.rag_id == rag_id, cycle.status == 'review'; очистка prod (DELETE WHERE); копирование -triples и -ontology в prod (get_ttl → put для tri, post для ont); удаление датасетов -tri, -ont, -stg (игнорируем ошибки); UploadCycle.status='merged', merged_at=now(); RagInstance.cycle_count += 1; ответ {message: "approved"}.

#### ✅ 4.4 `POST /rags/{rag_id}/chat` — RAG-вопрос

```python
class ChatRequest(BaseModel):
    question: str

@router.post("/{rag_id}/chat")
def chat(rag_id: int, body: ChatRequest, ...):
    # _can_access_rag
    # rag_context.build_context_a(question, dataset=rag.fuseki_dataset)
    #   — переиспользовать логику из graphrag-test/rag_context.py
    # rag_llm.ask_llm(context, question)
    #   — переиспользовать из graphrag-test/rag_llm.py
    # Вернуть {answer: str, context_used: int}
```

Зависимость: `graphrag-test/` должна быть на `sys.path` backend или иметь установленный пакет `graphrag_lib`. Для dev: добавить `sys.path.insert(0, "../../graphrag-test")` в `main.py` или конфигурацию. Запуск backend из корня репозитория (ferag/) обеспечивает корректный путь к graphrag-test; при запуске из `code/backend` либо запускать из корня (uvicorn из ferag с путём к app), либо вынести путь в конфиг (например, `graphrag_test_dir`).

**Исполнение:** В `app/config.py` добавлены `llm_api_url`, `llm_model`. В `main.py` в начало добавлен путь к `graphrag-test` (project_root = ...parent.parent.parent.parent, graphrag_test = project_root / "graphrag-test"). В `requirements.txt` — `requests`, `openai`. Эндпоинт `POST /rags/{rag_id}/chat` (ChatRequest.question): _can_access_rag; при отсутствии rag_context/rag_llm — 503; вызов build_context_by_question(question, url, auth, ds=rag.fuseki_dataset); context_used = len(context); get_llm_client(base_url=llm_api_url, ...), answer_from_context(context, question, client, model=llm_model); при ошибке LLM — 502; ответ ChatResponse(answer, context_used).

---

### ✅ 5. Alembic миграция

Поля `UploadCycle` и `Task` уже в моделях. Проверить, нет ли незамигрированных изменений. Если нет новых полей — миграция не нужна.

**Исполнение:** Сравнены `app/models.py` и единственная миграция `alembic/versions/8355f4b3f5a9_initial.py`. Таблицы `upload_cycles` (id, rag_id, cycle_n, status, created_at, merged_at) и `tasks` (id, rag_id, cycle_id, type, status, celery_task_id, error, created_at, updated_at) уже созданы в initial; расхождений с моделями нет. Дополнительная миграция не требуется.

---

### ✅ 6. Финальная проверка

Сценарий:
1. Redis на cr-ubu уже запущен (шаг 1.1); проверка: `redis-cli -h 10.7.0.1 -p 47379 ping` → `PONG`.
2. `docker compose up -d worker` — Worker поднят (nb-win).
3. `POST /auth/login` → token.
4. `POST /rags` → rag_id.
5. `POST /rags/{id}/upload` (файл `graphrag-test/input/source.txt`) → `{cycle_id, task_id}`.
6. WebSocket `ws://localhost:47821/ws/tasks/{task_id}?token=...` — получить статусы шагов.
7. Дождаться `{"status": "done"}`.
8. `POST /rags/{id}/cycles/{cycle_id}/approve` → 200.
9. Fuseki: датасет `ferag-{id:05d}` содержит триплеты.
10. `POST /rags/{id}/chat` с вопросом → LLM-ответ.

**Исполнение (19.02.2026):** Финальная проверка пройдена полностью. Скрипт `scripts/run_final_check.py` выполнен с флагом `FERAG_SOURCE_FILE=scripts/test_source.txt` (короткий тестовый текст для ускорения LLM). Все 8 шагов успешны: health → register/login → create RAG (id=22) → upload (pipeline запущен) → polling status=done → approve → cycle_count=1 → chat: LLM ответил *"Alice Smith - это инженер-программист, работающий в корпорации ACME..."*.

Исправления, потребовавшиеся в процессе:
- `deploy/nb-win/docker-compose.yml`: порт LM Studio исправлен `41234 → 1234`; добавлен volume `/tmp/ferag:/tmp/ferag` (shared work dir c backend); 
- `code/worker/tasks/base.py`: сигнатура `on_chain_failure` исправлена с `(request, excinfo)` на `(request, exc, traceback)` (Celery передаёт 3 аргумента);
- `code/worker/tasks/graphrag_task.py`: `_prepare_work_dir` — пропускает `shutil.copy2` когда src == dst; `_write_settings_yaml` — поддержка схемы graphrag 3.x (`completion_models` вместо `models`); в subprocess добавлен флаг `--skip-validation`;
- `graphrag-test/settings.yaml`: полностью переписан под graphrag 3.0.2 — `completion_models`, `embedding_models` (nomic-embed-text), `cache.type: none`, `embed_text.names: []`, `max_tokens: 4096`, `max_retries: 3`, `max_gleanings: 0`;
- `graphrag-test/prompts/`: промпты обновлены до формата graphrag 3.0.2 (жёсткие разделители `<|>`, `##`, `<|COMPLETE|>` вместо шаблонных переменных);
- `scripts/run_final_check.py`: таймаут polling увеличен до 2 часов (для медленной 70B модели).

Запуск: `FERAG_API=http://127.0.0.1:47824 FERAG_SOURCE_FILE=scripts/test_source.txt python scripts/run_final_check.py`

---

## Критерии успеха (Чат 2)

- ✅ Redis на cr-ubu отвечает на `ping` через WireGuard; `docker compose up -d worker` (nb-win) запускается без ошибок.
- ✅ `POST /rags/{id}/upload` возвращает `{cycle_id, task_id}`.
- ✅ WebSocket `/ws/tasks/{id}` стримит статусы всех шагов (5 шагов).
- ✅ После approve: `GET /rags/{id}` → `cycle_count == 1`.
- ✅ После approve: датасет `ferag-{id:05d}` содержит триплеты из загруженного файла (проверить через Fuseki SPARQL).
- ✅ `POST /rags/{id}/chat` → ответ LLM, не пустой.

---

## Вне объёма данного плана

- Управление участниками (`POST /rags/{id}/members`) — Чат 3.
- Frontend (Vue 3) — Чат 3.
- Docker-деплой на cr-ubu — Чат 3.
- `code/backend/Dockerfile` (uvicorn) — Чат 3.

---

## Контур следующих чатов

### Чат 3 — Frontend (Vue) + деплой

(описан в [26-0218-1703_plan.md](26-0218-1703_plan.md), раздел «Чат 3»)

---

## Ссылки

- Архитектура: [docs/project/PROJECT-004.md](../project/PROJECT-004.md)
- Задачи: [docs/tasks/TASKS.md](../tasks/TASKS.md) (Блок 9, задачи 9.2–9.5)
- Docker: [deploy/nb-win/docker-compose.yml](../../deploy/nb-win/docker-compose.yml)
- Предшественник: [26-0218-1703_plan.md](26-0218-1703_plan.md)
- Лабораторный pipeline: [graphrag-test/README_RAG.md](../../graphrag-test/README_RAG.md)
