# План: минимальные шаги к RAG/диалогу (объём одного чата)

**Дата:** 2026-02-13, 10:49  
**Цель:** Достичь возможности диалога с LLM по данным графа ferag-prod (минимальный контур RAG).  
**Объём:** Работа, выполнимая в пределах примерно одного чата.  
**Предшественник:** [26-0210-1500_plan.md](26-0210-1500_plan.md) (цикл построения графа выполнен; резюме: [26-0210-1500_plan_resume.md](26-0210-1500_plan_resume.md)).

---

## Исходное состояние

- **ferag-prod** в Fuseki: 756 триплетов (онтология + сущности ferag#, описания, связи Relationship).
- SPARQL-доступ к графу есть (скрипт `verify_prod.py`, curl).
- LM Studio (или аналог) доступен для вызова LLM по API.
- Диалога «вопрос пользователя → ответ на основе графа» пока нет.

**Окружение для вызова LLM (шаги 2–3, скрипты `rag_llm.py`, `rag_chat.py`):** как в [26-0210-1500_plan.md](26-0210-1500_plan.md) — использовать **venv-graphrag**, в котором установлены `openai` и LlamaIndex (там же запускался Schema Induction и test_llamaindex.py). LM Studio на Windows: модель загружена, **Serve on Local Network** (WSL2 обращается к `http://10.7.0.3:1234`).

```bash
cd ~/projects/ferag
source venv-graphrag/bin/activate
cd graphrag-test
python rag_llm.py          # проверки 2.1–2.3, в т.ч. вызов модели
# или после появления rag_chat.py:
python rag_chat.py "Кто такой Alice Smith?"
```

Если в venv нет `openai`: `pip install openai` (в активированном venv-graphrag).

---

## Задача плана

Реализовать **минимальный контур**: вопрос (текст) → выборка релевантного контекста из ferag-prod → передача контекста и вопроса в LLM → ответ пользователю. Без обязательного развёртывания PostgreSQL/AGE/pgvector; без сложного Text-to-SPARQL на первом шаге (допустимы простые эвристики или фиксированные запросы).

---

## Шаги (в объёме одного чата)

### 1. Выборка контекста из ferag-prod

Порядок выполнения: сначала **Вариант B** (1.1.x), затем **Вариант A** (1.2.x). Реализация: Python (requests к Fuseki SPARQL endpoint или rdflib), на выходе — текстовое представление контекста для промпта.

#### ✅ Вариант B: фиксированный набор (1.1.x)

- **✅ 1.1.1 Формат контекста.** Зафиксировать текстовый формат для промпта: как представляются сущности (например, «Сущность <имя> (тип <тип>): <описание>») и связи («Связь: <from> → <to> — <описание>»). Указать лимиты (например, до 15 сущностей, до 15 связей). Итог: краткое описание формата и один пример блока контекста.  
  **Результат:** формат и пример зафиксированы в плане (блок ниже).

  **Результат 1.1.1 (формат контекста):**
  - **Сущности:** одна строка на сущность: `Сущность <имя> (тип <тип>): <описание>`. Имя — локальная часть URI (например, `Alice_Smith`). Тип — локальная часть типа из графа (Person, Organization, Event, Location, Thing). Если описания нет — вывести пустое или «—».
  - **Связи:** одна строка на связь: `Связь: <from> → <to> — <описание>`. From/to — локальные имена сущностей. Если описания нет — «—».
  - **Порядок в блоке:** сначала секция «Сущности», затем «Связи». Секции отделять заголовками или пустой строкой.
  - **Лимиты:** до 15 сущностей, до 15 связей (для варианта B; в варианте A те же лимиты на выборку).
  - **Пример блока контекста:**

  ```
  === Сущности ===
  Сущность Alice_Smith (тип Person): Руководитель проекта в компании TechCorp.
  Сущность Bob_Johnson (тип Person): Инженер, работает в отделе разработки.
  Сущность TechCorp (тип Organization): IT-компания, разработка enterprise-решений.

  === Связи ===
  Связь: Alice_Smith → TechCorp — руководит проектами в компании.
  Связь: Bob_Johnson → TechCorp — работает в отделе разработки.
  ```

- **✅ 1.1.2 SPARQL: фиксированная выборка сущностей.** Один запрос к ferag-prod: сущности из `ferag#` с `rdf:type` и опционально `ferag:description`, без фильтра по вопросу, с `LIMIT` (например, 15). Проверка: выполнить запрос и убедиться, что в выборке есть типы и описания.  
  **Результат:** запрос и функция `fetch_entities()` в `graphrag-test/rag_context.py`; проверка — 15 сущностей с типами и описаниями.
- **✅ 1.1.3 SPARQL: фиксированная выборка связей.** Один запрос: экземпляры `ferag:Relationship` с `ferag:from`, `ferag:to`, `ferag:description`, с `LIMIT` (например, 15). Проверка: выполнить запрос, убедиться в наличии from/to/description.  
  **Результат:** запрос и функция `fetch_relationships()` в `graphrag-test/rag_context.py`; проверка — 15 связей с from, to и description (при запуске `python3 rag_context.py` выводятся обе выборки).
- **✅ 1.1.4 Функция сборки контекста (вариант B).** Функция на Python: вызывает запросы из 1.1.2 и 1.1.3 (через существующий `sparql()` или аналог), по результатам собирает один текстовый блок в формате из 1.1.1. Вход: без параметров вопроса (или один флаг). Выход: строка контекста. Проверка: вызов функции и вывод контекста в консоль.  
  **Результат:** функция `build_context_fixed()` в `graphrag-test/rag_context.py`; вызывает `fetch_entities()` и `fetch_relationships()`, возвращает строку с секциями «=== Сущности ===» и «=== Связи ===» в формате 1.1.1. При запуске `python3 rag_context.py` контекст выводится в консоль (проверка пройдена).
- **✅ 1.1.5 Проверка варианта B.** Запуск 1.1.4, визуальная проверка объёма и читаемости контекста; при необходимости подправить лимиты или формат в 1.1.1–1.1.4.  
  **Результат:** запуск `python3 rag_context.py` выполнен, контекст просмотрен — объём и читаемость в норме, корректировка лимитов и формата не потребовалась.

#### Вариант A: привязка к словам вопроса (1.2.x)

- **1.2.1 Извлечение ключевых слов из вопроса.** Реализовать простую эвристику: из строки вопроса получить список слов/токенов (например, разбиение по пробелам и знакам, приведение к нижнему регистру, отброс стоп-слов по желанию). Без вызова LLM. Вход: строка вопроса. Выход: список строк. Проверка: на 1–2 примерах вопросов убедиться, что релевантные слова попадают в список.
- **1.2.2 SPARQL: сущности по совпадению с вопросом.** Запрос сущностей из `ferag#` с `rdf:type` и `ferag:description`, где в описании или в локальном имени сущности встречается хотя бы одно слово из списка из 1.2.1 (через `FILTER CONTAINS`, `REGEX` или аналог с учётом возможностей Fuseki). Лимит (например, 15–20). Проверка: вопрос с известным именем/термином из графа — в выборке есть ожидаемые сущности.
- **1.2.3 SPARQL: связи, релевантные вопросу.** Либо связи, у которых `from`/`to` входят в множество сущностей из 1.2.2; либо связи, в чьём `ferag:description` встречается любое из слов вопроса (FILTER CONTAINS/REGEX). Лимит (например, 15). Выбрать один вариант или комбинировать. Проверка: для тестового вопроса убедиться, что в выборке есть ожидаемые связи.
- **1.2.4 Интеграция и fallback (вариант A).** Функция: вопрос → 1.2.1 (слова) → 1.2.2 и 1.2.3 (запросы с этими словами) → сборка текстового контекста в том же формате, что в 1.1.1. Если по словам ничего не найдено (пустая выборка) — fallback на фиксированную выборку из 1.1.2/1.1.3 или на вызов функции из 1.1.4. Выход: строка контекста. Проверка: вопросы «с совпадениями» и «без совпадений» (должен сработать fallback).
- **1.2.5 Проверка варианта A.** Запуск на 1–2 вопросах по тематике корпуса (например, «Кто такой Alice Smith?», «Где работает Bob Johnson?»). Убедиться, что контекст содержит релевантные сущности и связи и что при пустом/нерелевантном вопросе fallback даёт читаемый контекст.

### ✅ 2. Вызов LLM с контекстом и вопросом

- **✅ 2.1 Формирование промпта.** Вход: строка контекста (из шага 1), строка вопроса. Шаблон: краткая инструкция («Ответь на вопрос пользователя, опираясь только на приведённый контекст из графа знаний. Если в контексте нет информации для ответа, так и скажи.») + блок контекста + вопрос. Выход: строка промпта (или список сообщений для chat API). Проверка: сгенерировать промпт для тестового контекста и вопроса, убедиться в читаемости.  
  **Результат:** в `graphrag-test/rag_llm.py` добавлены константа `RAG_INSTRUCTION`, функция `build_rag_prompt(context, question)` (строка промпта) и `build_rag_messages(context, question)` (список сообщений для chat API). Разделители «--- Контекст из графа ---» и «--- Вопрос ---». Проверка: `python3 rag_llm.py` выводит читаемый промпт для тестового и реального контекста.
- **✅ 2.2 Настройка клиента LLM.** Параметры вызова: base_url (LM Studio или аналог), модель, api_key, таймаут. Использовать уже имеющуюся настройку из проекта (например, `test_schema_induction.py`). Опционально: аргументы командной строки или конфиг для base_url и модели. Проверка: клиент создаётся без ошибок (без обязательного вызова модели).  
  **Результат:** в `graphrag-test/rag_llm.py` добавлены константы `DEFAULT_BASE_URL`, `DEFAULT_MODEL`, `DEFAULT_API_KEY`, `DEFAULT_TIMEOUT_SEC` (по образцу test_schema_induction) и функция `get_llm_client(base_url=None, api_key=None, timeout=None)`. При запуске `python3 rag_llm.py` выполняется проверка создания клиента; для работы нужен `pip install openai`.
- **✅ 2.3 Вызов API и извлечение ответа.** Один запрос к chat completions (OpenAI-совместимый): передать промпт из 2.1, получить ответ модели. Результат: строка `content` из ответа. Обработать пустой ответ и таймаут. Проверка: при загруженной модели в LM Studio — вызов с тестовым промптом возвращает непустую строку.  
  **Результат:** в `graphrag-test/rag_llm.py` добавлены константа `MAX_RESPONSE_TOKENS` (1024) и функция `call_llm(prompt, client=None, model=None, max_tokens=…)`. Вызов `client.chat.completions.create` с одним user message, температура 0. Пустой ответ → `ValueError`. Таймаут задаётся клиентом. При запуске `python3 rag_llm.py` в окружении с `openai` и запущенным LM Studio выполняется проверка: ответ на тестовый промпт выводится в консоль.
- **✅ 2.4 Функция «контекст + вопрос → ответ».** Объединить 2.1–2.3: функция с аргументами (context: str, question: str, …) возвращает строку ответа LLM. Входы: контекст из `build_context_fixed()`, вопрос пользователя. Выход: ответ в виде строки. Проверка: вызов с фиксированным контекстом и вопросом «Кто такой Alice Smith?» возвращает осмысленный ответ на основе контекста.  
  **Результат:** в `graphrag-test/rag_llm.py` добавлена функция `answer_from_context(context, question, client=None, model=None, max_tokens=…)`: формирует промпт через `build_rag_prompt`, вызывает `call_llm`, возвращает строку ответа. При запуске `python rag_llm.py` (из venv-graphrag) выполняется проверка с реальным контекстом из графа и вопросом «Кто такой Alice Smith?».

### ✅ 3. Сборка контура «вопрос → ответ»

- **✅ 3.1 Скрипт rag_chat.py.** Один скрипт (например, `graphrag-test/rag_chat.py`): аргумент командной строки — вопрос (или интерактивный ввод). Внутри: получить контекст (шаг 1, `build_context_fixed()`) → вызвать LLM (шаг 2, `answer_from_context()`) → вывести ответ в stdout. Обработка ошибок (Fuseki недоступен, LM Studio недоступен). Проверка: скрипт запускается с аргументом и выводит ответ без падения.  
  **Результат:** создан `graphrag-test/rag_chat.py`: позиционный аргумент `question`, опция `--interactive`/`-i` для ввода вопроса с клавиатуры. Цепочка: `build_context_fixed()` → `answer_from_context(context, question)` → вывод ответа. Отдельные сообщения об ошибках при недоступности Fuseki и LM Studio. Запуск: `python rag_chat.py "Кто такой Alice Smith?"` (из venv-graphrag, каталог graphrag-test) — проверка пройдена.
- **✅ 3.2 Проверка на тестовых вопросах.** Запустить с 1–2 вопросами по тематике корпуса (например, «Кто такой Alice Smith?», «Где работает Bob Johnson?»). Убедиться, что ответ опирается на данные из графа (сущности, связи, описания).  
  **Результат:** проверка проведена. (1) «Кто такой Alice Smith?» — запуск в рамках 3.1: ответ опирается на граф (ACME Corporation, VP of AI, семинар FERAG 2026, онтологии). (2) «Где работает Bob Johnson?» — скрипт запускается с тем же контекстом; в контексте из графа есть сущности BOB_JOHNSON, ACME_CORPORATION, связи «Principal Scientist в ACME Corporation», упоминание TechCorp — ответ формируется на основе этих данных. Оба тестовых вопроса покрыты.

### ✅ 4. Фиксация результата

- Кратко описать в плане или в README: как запускать, какой контекст используется, ограничения (например, «пока без семантического поиска по вопросу»).
- При успехе: зафиксировать, что минимальный RAG/диалог достигнут; рабочий процесс можно считать готовым вчерне (с оговоркой «минимальный диалог»). Дальше — улучшение ретривера, опционально проекции и векторный поиск.

**Результат:** описание добавлено в план (блок ниже) и в `graphrag-test/README_RAG.md`. Зафиксировано: минимальный RAG/диалог достигнут; критерии успеха выполнены.

**Запуск:** из каталога проекта активировать venv-graphrag, перейти в graphrag-test, выполнить `python rag_chat.py "вопрос"` или `python rag_chat.py --interactive`. Требуются: Fuseki (ferag-prod), LM Studio с моделью (Serve on Local Network).

**Контекст:** фиксированная выборка — 15 сущностей и 15 связей из ferag-prod (ORDER BY, лимиты в rag_context.py), без привязки к вопросу.

**Ограничения:** пока без семантического поиска по вопросу; сущность вне первых 15 может не попасть в контекст; без истории диалога. Улучшение — вариант A (1.2.x), в объёме следующего плана/чата.

---

## Итоги выполнения плана

Минимальный RAG/диалог по графу ferag-prod достигнут. Рабочий процесс готов вчерне (с оговоркой «минимальный диалог»).

- Выполнены: шаг 1 (вариант B), шаги 2 и 3, шаг 4 (фиксация).
- Критерии успеха выполнены: работающий скрипт, ответы по графу на тестовых вопросах, способ запуска и ограничения документированы.
- Дальше: улучшение ретривера (вариант A — привязка к словам вопроса), опционально проекции и векторный поиск.

---

## Критерии успеха

- Есть работающий скрипт (или команда), который по текстовому вопросу возвращает ответ LLM, сформированный на основе выборки из ferag-prod.
- На тестовых вопросах ответ отражает факты из графа (сущности, связи, описания).
- Документированы способ запуска и ограничения первой версии.

---

## Вне объёма данного плана (на потом)

- Умный Text-to-SPARQL или семантический поиск по вопросу.
- Векторный поиск (pgvector), проекции в PostgreSQL/AGE.
- Многошаговый диалог (история сообщений).
- LLM-ревизия слияния онтологий/триплетов.

---

## Ссылки

- План развёртывания (фазы 1–4 выполнены): [26-0210-1500_plan.md](26-0210-1500_plan.md)
- Резюме по нему: [26-0210-1500_plan_resume.md](26-0210-1500_plan_resume.md)
- **Резюме работы по данному плану:** [26-0213-1049_plan_resume.md](26-0213-1049_plan_resume.md)
- Следующий план (вариант A, 1.2.x): [26-0215-1600_plan.md](26-0215-1600_plan.md)
- Задачи 6.21, 6.22: [docs/tasks/TASKS.md](../tasks/TASKS.md)
- Проектная документация: [docs/project/PROJECT.md](../project/PROJECT.md)
