# Чат 3 — Frontend (Vue) + деплой

**Дата:** 2026-02-19 (план подготовлен в конце Чата 2)  
**Предшественник:** [26-0219-1110_plan.md](26-0219-1110_plan.md) (backend + worker + полный цикл — все критерии выполнены).

---

## Исходное состояние

- **Backend**: FastAPI; все эндпоинты работают (auth, rags, upload, WebSocket, approve, chat, tasks); запускается вручную (`uvicorn`) из `code/backend/`.
- **Worker**: Celery; запущен в Docker на nb-win; цепочка задач (`run_graphrag` → `schema` → `merge` → `staging`) работает.
- **Redis**: запущен на cr-ubu (`ferag-redis`, `10.7.0.1:47379`).
- **PostgreSQL + Fuseki**: запущены в Docker на nb-win.
- **code/backend/Dockerfile**: отсутствует.
- **code/frontend/**: отсутствует.
- **cr-ubu**: только Redis-контейнер; backend не задеплоен; Nginx не настроен под ferag.
- **`/var/www/ferag/`** на cr-ubu: не создан.

---

## Задача плана

1. Добавить API управления участниками RAG (`/rags/{id}/members`).
2. Написать `code/backend/Dockerfile` для деплоя FastAPI на cr-ubu.
3. Решить задачу совместного доступа к `work_dir` между backend (cr-ubu) и worker (nb-win).
4. Реализовать `code/frontend/` — Vue 3 + Vite SPA.
5. Задеплоить backend в Docker на cr-ubu.
6. Собрать и доставить frontend на cr-ubu (`/var/www/ferag/`).
7. Подключить Nginx на cr-ubu.
8. Финальная проверка: `https://ontoline.ru/ferag/` → register → create RAG → upload → LLM-ответ.

---

## Исходное состояние (файлы и сервисы)

```
ferag/
├── code/
│   ├── backend/           # ✅ Готово (Чаты 1–2)
│   ├── worker/            # ✅ Готово (Чат 2)
│   └── frontend/          # ❌ Не существует
└── deploy/
    ├── cr-ubu/
    │   ├── docker-compose.yml   # ✅ redis + ferag; ferag не собран
    │   └── nginx-location-ferag.conf  # ✅ конфиг готов; не подключён
    └── nb-win/
        └── docker-compose.yml   # ✅ postgres + fuseki + worker
```

**Сервисы перед началом:**

| Сервис | Статус | Адрес |
|--------|--------|-------|
| PostgreSQL | ✅ запущен | `localhost:45432` / `10.7.0.3:45432` |
| Fuseki | ✅ запущен | `localhost:43030` |
| Redis | ✅ запущен (cr-ubu) | `10.7.0.1:47379` |
| Worker | ✅ запущен (Docker, nb-win) | — |
| Backend | ⚡ запускается вручную | `localhost:47821` |
| ferag-контейнер (cr-ubu) | ❌ не собран | — |
| Nginx `/ferag/` | ❌ не подключён | — |
| LM Studio | ⚡ по необходимости | `localhost:1234` |

---

## Архитектурное решение: work_dir при кросс-машинном деплое

**Проблема:** backend (cr-ubu) сохраняет загруженный файл в `work_dir` (`/tmp/ferag/rag_N/cycle_M/input/source.txt`). Worker (nb-win) читает его оттуда же. В текущем виде они на разных машинах — volume-маппинг не работает.

**Принятое решение: хранение тела файла в PostgreSQL.**

В таблицу `upload_cycles` добавляется колонка `source_content TEXT` (или `BYTEA`). При upload backend сохраняет содержимое в БД. Worker в начале `run_graphrag` читает `source_content` из БД и записывает в локальный work_dir.

**Преимущества:** не требует общего файлового хранилища; работает через существующее WireGuard-соединение backend↔PostgreSQL; простая реализация.

---

## Шаги

### ✅ 1. API управления участниками RAG

Таблица `rag_members` уже существует в схеме (`id, rag_id, user_id, role`). Нужно добавить эндпоинты в `code/backend/app/routers/rags.py`.

#### ✅ 1.1 `POST /rags/{rag_id}/members`

```python
class MemberAddBody(BaseModel):
    email: str
    role: str  # 'viewer' | 'editor'

@router.post("/{rag_id}/members", status_code=201)
def add_member(rag_id: int, body: MemberAddBody, ...):
    # Только owner может добавлять участников
    # Найти User по email (404 если нет)
    # Проверить: уже не owner и не member
    # Создать RagMember(rag_id, user_id=target.id, role=body.role)
    # Вернуть {user_id, email, role}
```

**Исполнение:** в `code/backend/app/routers/rags.py` добавлены модели `MemberAddBody` (email, role: Literal["viewer"|"editor"]) и `MemberResponse` (user_id, email, role). Эндпоинт `POST /{rag_id}/members` проверяет доступ через `_can_access_rag` и `_is_owner`; ищет пользователя по email (404 при отсутствии); отклоняет добавление владельца и уже существующего участника (400); создаёт запись `RagMember` и возвращает 201 с телом `MemberResponse`.

#### ✅ 1.2 `GET /rags/{rag_id}/members`

```python
@router.get("/{rag_id}/members")
def list_members(rag_id: int, ...):
    # Вернуть список участников (user_id, email, display_name, role)
    # Доступен owner и любой member
```

**Исполнение:** в `code/backend/app/routers/rags.py` добавлена модель `MemberListItem` (user_id, email, display_name, role) и эндпоинт `GET /{rag_id}/members`. Доступ проверяется через `_can_access_rag` (владелец или участник). В ответе сначала возвращается владелец с role="owner", затем все записи из `RagMember` с подставленными из `User` email и display_name.

#### ✅ 1.3 `DELETE /rags/{rag_id}/members/{user_id}`

```python
@router.delete("/{rag_id}/members/{user_id}", status_code=204)
def remove_member(rag_id: int, user_id: int, ...):
    # Только owner; нельзя удалить самого себя (он owner)
    # Удалить запись RagMember
```

**Исполнение:** в `code/backend/app/routers/rags.py` добавлен эндпоинт `DELETE /{rag_id}/members/{user_id}`. Доступ только у владельца (`_is_owner`); при попытке удалить владельца (user_id == rag.owner_id) возвращается 400; при отсутствии записи в `RagMember` — 404. Удаляется запись `RagMember(rag_id, user_id)`, ответ 204.

---

### ✅ 2. Alembic: миграция `source_content`

Добавить поле в модель `UploadCycle`:

```python
source_content: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
```

Сгенерировать и применить миграцию:

```bash
cd code/backend
alembic revision --autogenerate -m "add_upload_cycle_source_content"
alembic upgrade head
```

**Исполнение:** в `code/backend/app/models.py` в модель `UploadCycle` добавлено поле `source_content: Mapped[Optional[str]] = mapped_column(Text, nullable=True)`. Создана миграция `alembic/versions/1316c47e2e74_add_upload_cycle_source_content.py` (добавление колонки `upload_cycles.source_content`), выполнена `alembic upgrade head` — миграция применена к БД.

---

### ✅ 3. Backend: адаптация upload для кросс-машинного деплоя

#### ✅ 3.1 Сохранение содержимого в БД

В `POST /rags/{id}/upload` добавить запись содержимого файла в `UploadCycle.source_content` (вместо/вместе с записью на диск):

```python
content = await file.read()
cycle.source_content = content.decode("utf-8")
db.commit()
```

Локальная запись на диск остаётся для обратной совместимости при dev-запуске.

**Исполнение:** в `code/backend/app/routers/rags.py` в обработчике upload после `content = await file.read()` добавлено присвоение `cycle.source_content = content.decode("utf-8")`; значение сохраняется в БД при существующем `db.commit()`. Запись в work_dir оставлена без изменений.

#### ✅ 3.2 Worker: чтение из БД

В `tasks/graphrag_task.py` в `_prepare_work_dir`:

```python
# Если файл не существует локально — читаем из БД
if not input_path.exists():
    with get_db_session() as db:
        cycle = db.get(UploadCycle, cycle_id)
        if cycle and cycle.source_content:
            input_path.parent.mkdir(parents=True, exist_ok=True)
            input_path.write_text(cycle.source_content)
        else:
            raise FileNotFoundError(f"source not found for cycle {cycle_id}")
```

**Исполнение:** в `code/worker/tasks/base.py` добавлена функция `get_cycle_source_content(cycle_id)` (raw SQL `SELECT source_content FROM upload_cycles`). В `graphrag_task.py` у `_prepare_work_dir` добавлен аргумент `cycle_id`; при отсутствии файла по пути `input_file` вызывается `get_cycle_source_content`, при пустом результате — `FileNotFoundError`, иначе содержимое записывается в `work_dir/input/source.txt`. Вызов из `run_graphrag` обновлён.

---

### ✅ 4. `code/backend/Dockerfile`

```dockerfile
FROM python:3.11-slim

RUN apt-get update && apt-get install -y build-essential curl \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . /app

# Worker пакет нужен для отправки задач в Celery (start_update_chain)
# Добавляется через volume в docker-compose (CELERY_TASKS_ONLY=1)
# или через COPY ниже для standalone-деплоя:
# COPY ../worker /app/worker_pkg
# ENV PYTHONPATH=/app:/app/worker_pkg

ENV PYTHONPATH=/app

EXPOSE 47821
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "47821"]
```

**Зависимость от worker-пакета:** при отправке Celery-задач backend импортирует `from worker.tasks import start_update_chain`. На cr-ubu этот пакет недоступен без явной копии. Решение: в `deploy/cr-ubu/docker-compose.yml` для сервиса `ferag` добавить volume, монтирующий `code/worker` в контейнер как `/app/worker`, и обновить PYTHONPATH.

Альтернативное решение (более чистое): заменить прямой импорт на `celery.send_task` по имени строки, избавившись от зависимости на файловую систему worker-пакета.

**Рекомендованный путь:** использовать `send_task` по имени.

В `code/backend/app/routers/rags.py` заменить:

```python
from worker.tasks import start_update_chain
start_update_chain(rag_id, cycle.id, task.id, str(file_path))
```

На:

```python
from app.celery_sender import send_update_chain
send_update_chain(rag_id, cycle.id, task.id, str(file_path))
```

Создать `code/backend/app/celery_sender.py`:

```python
from celery import Celery
from app.config import get_settings

def _get_celery() -> Celery:
    s = get_settings()
    return Celery(broker=s.celery_broker_url, backend=s.celery_result_backend)

def send_update_chain(rag_id, cycle_id, task_id, input_file):
    app = _get_celery()
    from celery import chain, signature
    chain(
        app.signature("worker.tasks.graphrag_task.run_graphrag",
                      args=[rag_id, cycle_id, task_id, input_file]),
        app.signature("worker.tasks.schema_task.run_schema_induction",
                      args=[rag_id, cycle_id, task_id], immutable=True),
        app.signature("worker.tasks.merge_task.do_merge",
                      args=[rag_id, cycle_id, task_id], immutable=True),
        app.signature("worker.tasks.staging_task.load_to_staging",
                      args=[rag_id, cycle_id, task_id], immutable=True),
    ).apply_async()
```

**Исполнение:** создан `code/backend/Dockerfile` (python:3.11-slim, requirements, CMD uvicorn). Реализован рекомендованный путь: в `app/config.py` добавлены `celery_broker_url` и `celery_result_backend`; создан `app/celery_sender.py` с `send_update_chain` (цепочка через `app.signature` по имени задач); в `rags.py` убран импорт worker и sys.path, вызов заменён на `send_update_chain`. Добавлен `.dockerignore` (venv, .env, __pycache__).

---

### ✅ 5. `deploy/cr-ubu/docker-compose.yml`: ferag-контейнер

Файл уже содержит сервис `ferag`; нужно убедиться в корректности:

```yaml
ferag:
  build:
    context: ../../code/backend
    dockerfile: Dockerfile
  container_name: ferag
  restart: unless-stopped
  ports:
    - "127.0.0.1:47821:47821"
  environment:
    - DATABASE_URL=postgresql://ferag:${DB_PASSWORD}@10.7.0.3:45432/ferag_app
    - REDIS_URL=redis://redis:47379/0
    - CELERY_BROKER_URL=redis://redis:47379/0
    - CELERY_RESULT_BACKEND=redis://redis:47379/1
    - JWT_SECRET=${JWT_SECRET}
    - FUSEKI_URL=http://10.7.0.3:43030
    - FUSEKI_USER=admin
    - FUSEKI_PASSWORD=${FUSEKI_PASSWORD}
    - ALLOWED_ORIGINS=https://ontoline.ru
  depends_on:
    - redis
  networks:
    - ferag-network
```

**Важно:** Fuseki на nb-win сейчас слушает только на `127.0.0.1:43030`. Для доступа с cr-ubu нужно также открыть `10.7.0.3:43030` в `deploy/nb-win/docker-compose.yml`:

```yaml
ports:
  - "127.0.0.1:43030:3030"   # localhost (worker внутри Docker-сети)
  - "10.7.0.3:43030:3030"    # WireGuard → backend на cr-ubu
```

**Исполнение:** в `deploy/cr-ubu/docker-compose.yml` для сервиса `ferag` добавлены переменные `CELERY_BROKER_URL=redis://redis:47379/0` и `CELERY_RESULT_BACKEND=redis://redis:47379/1`. В `deploy/nb-win/docker-compose.yml` для сервиса `fuseki` добавлен проброс порта `10.7.0.3:43030:3030` для доступа с cr-ubu по WireGuard.

---

### 6. `code/frontend/` — Vue 3 + Vite

#### ✅ 6.1 Инициализация проекта

```bash
cd code
npm create vue@latest frontend
# Options: TypeScript ✅, Vue Router ✅, Pinia ✅, ESLint ✅
cd frontend
npm install axios
```

`vite.config.ts`:

```typescript
import { defineConfig } from 'vite'
import vue from '@vitejs/plugin-vue'

export default defineConfig({
  plugins: [vue()],
  base: '/ferag/',
  server: {
    proxy: {
      '/ferag/api': { target: 'http://localhost:47821', rewrite: p => p.replace('/ferag/api', '') },
      '/ferag/ws': { target: 'ws://localhost:47821', ws: true, rewrite: p => p.replace('/ferag/ws', '/ws') },
    }
  }
})
```

**Исполнение:** в `code/` выполнен `npm create vue@latest frontend -- --typescript --router --pinia --eslint`, установлены зависимости и axios. В `vite.config.ts` добавлены `base: '/ferag/'` и proxy для `/ferag/api` и `/ferag/ws` на localhost:47821.

#### ✅ 6.2 Структура

```
code/frontend/src/
├── api/
│   ├── client.ts       # axios instance, baseURL='/ferag/api/', JWT interceptor
│   ├── auth.ts         # register, login, me
│   ├── rags.ts         # listRags, createRag, getRag, deleteRag
│   ├── upload.ts       # uploadFile → {cycle_id, task_id}
│   ├── tasks.ts        # getTask
│   ├── approve.ts      # approveCycle
│   ├── chat.ts         # sendQuestion → {answer, context_used}
│   └── members.ts      # listMembers, addMember, removeMember
├── stores/
│   ├── auth.ts         # useAuthStore: token, user, login(), logout()
│   └── rags.ts         # useRagsStore: list, current
├── router/
│   └── index.ts        # маршруты (guards: requireAuth)
├── views/
│   ├── LoginView.vue       # /ferag/login — форма входа/регистрации
│   ├── RagsView.vue        # /ferag/ — список RAG
│   ├── RagDetailView.vue   # /ferag/rags/:id — карточка RAG + вкладки
│   ├── UploadView.vue      # /ferag/rags/:id/upload — загрузка + прогресс
│   ├── ChatView.vue        # /ferag/rags/:id/chat — диалог
│   └── MembersView.vue     # /ferag/rags/:id/members — участники
└── components/
    ├── NavBar.vue
    ├── TaskProgress.vue    # WebSocket-прогресс задачи (шаги pipeline)
    └── MessageBubble.vue   # пузырь вопрос/ответ
```

**Исполнение:** созданы каталоги и файлы по структуре плана: api/ (client с JWT и baseURL /ferag/api, auth, rags, upload, tasks, approve, chat, members), stores/ (auth с token/user/login/logout, rags с list/current), router с маршрутами и guard requireAuth, views (LoginView, RagsView, RagDetailView с вкладками, UploadView с TaskProgress и approve, ChatView с MessageBubble, MembersView), components (NavBar, TaskProgress с WebSocket по taskId, MessageBubble). App.vue упрощён до RouterView. Сборка npm run build успешна.

#### ✅ 6.3 WebSocket в `TaskProgress.vue`

```typescript
const ws = new WebSocket(
  `wss://ontoline.ru/ferag/ws/tasks/${taskId}?token=${authStore.token}`
)
ws.onmessage = (e) => {
  const msg = JSON.parse(e.data)   // {status, step, error}
  steps.value.push(msg)
  if (msg.status === 'done') emit('done')
  if (msg.status === 'failed') emit('failed', msg.error)
}
```

При dev-запуске: `ws://localhost:47821/ws/tasks/{id}?token=…`.

**Исполнение:** в `TaskProgress.vue` уже реализовано при создании структуры (6.2): URL формируется динамически — в dev `ws://localhost:47821/ws/tasks/{id}?token=…`, в prod — текущий host и путь `/ferag/ws/tasks/{id}?token=…`. В `onmessage` парсится JSON `{status, step, error}`, шаги пишутся в `steps`, при `done`/`failed` эмитятся события. Добавлен комментарий в коде.

#### ✅ 6.4 Сборка

```bash
cd code/frontend
npm run build   # → dist/
```

Артефакты: `code/frontend/dist/` (index.html + assets/). Vite вставляет `base='/ferag/'` в пути ресурсов.

**Исполнение:** выполнена команда `npm run build` в `code/frontend`; артефакты в `dist/` (index.html и assets/*.js, *.css). Пути в index.html используют префикс `/ferag/` (vite.config.ts base).

---

### ✅ 7. Деплой frontend на cr-ubu

```bash
# На nb-win (WSL2)
rsync -avz code/frontend/dist/ user@cr-ubu:/var/www/ferag/
# или через SCP:
scp -r code/frontend/dist/* user@cr-ubu:/var/www/ferag/
```

На cr-ubu:

```bash
sudo mkdir -p /var/www/ferag
sudo chown www-data:www-data /var/www/ferag
```

**Исполнение:** каталог `/var/www/ferag` создан на cr-ubu (176.108.244.252), владелец www-data. С nb-win выполнен rsync в `/tmp/ferag-dist/` под user1 (после настройки SSH-ключа alex@nb-win → user1), затем на сервере — `sudo rsync -a /tmp/ferag-dist/ /var/www/ferag/` и `sudo chown -R www-data:www-data /var/www/ferag`. В каталоге разложены index.html, favicon.ico, assets/ (JS/CSS).

---

### ✅ 8. Деплой backend-контейнера на cr-ubu

На cr-ubu необходим доступ к исходному коду (`code/backend/`). Варианты:

**Вариант A (рекомендован для lab):** клонировать репозиторий на cr-ubu, собирать образ там.

```bash
# На cr-ubu
git clone <repo> /opt/ferag
cd /opt/ferag/deploy/cr-ubu
cp .env.example .env  # заполнить секреты
docker compose up -d --build ferag
```

**Вариант B:** собирать образ на nb-win, экспортировать через `docker save`, заливать на cr-ubu.

```bash
# nb-win
docker build -t ferag-backend:latest code/backend/
docker save ferag-backend:latest | gzip > ferag-backend.tar.gz
scp ferag-backend.tar.gz cr-ubu:/tmp/
# cr-ubu
docker load < /tmp/ferag-backend.tar.gz
docker compose up -d ferag
```

**Исполнение (вариант B, 2026-02-20).**

Сборка на nb-win: `docker build -t ferag-backend:latest -f code/backend/Dockerfile code/backend/`, экспорт `docker save ferag-backend:latest | gzip > /tmp/ferag-backend.tar.gz`. Загрузка на cr-ubu: `scp` под пользователем **cursor-agent** (ключ проекта `.ssh/cursor_agent_key`, хост `176.108.244.252`) в `/tmp/ferag-backend.tar.gz`.

На cr-ubu каталог `/opt/ferag` отсутствовал; развёртывание выполнено в домашнем каталоге cursor-agent: `~/ferag-deploy/` (docker-compose.yml, .env.example залиты через scp). Выполнены `docker load < /tmp/ferag-backend.tar.gz`, образ `ferag-backend:latest` загружен.

Контейнер **ferag-redis** уже был запущен ранее (сеть bridge, IP 172.17.0.2). Контейнер ferag запущен командой `docker run` с сетью bridge, порт 127.0.0.1:47821, переменные REDIS_URL/CELERY_* указывают на redis://172.17.0.2:47379/0 и …/1. В `deploy/cr-ubu/docker-compose.yml` добавлено `image: ferag-backend:latest` для варианта B.

Реальные секреты: .env с nb-win передан на сервер (`scp code/backend/.env user1@176.108.244.252:/tmp/ferag.env`), под user1 выполнены `sudo cp /tmp/ferag.env /home/cursor-agent/ferag-deploy/.env && sudo chown cursor-agent:cursor-agent ...`; контейнер ferag перезапущен с переменными из этого .env (`docker stop ferag && docker rm ferag`, затем `sudo -u cursor-agent bash -c 'cd ~/ferag-deploy && set -a && . ./.env && set +a && docker run -d ...'`).

Итог: backend и redis работают в двух контейнерах (ferag, ferag-redis). Обновление .env и перезапуск — по инструкции в [deploy/DEPLOYMENT_SUMMARY.md](../../deploy/DEPLOYMENT_SUMMARY.md#текущее-развёртывание-backend-на-cr-ubu-вариант-b).

---

### ✅ 9. Nginx на cr-ubu

```bash
# На cr-ubu
sudo cp /opt/ferag/deploy/cr-ubu/nginx-location-ferag.conf \
    /etc/nginx/snippets/ferag.conf

# В /etc/nginx/sites-available/ontoline.ru внутри server { listen 443 ssl ... }
# добавить строку:
#     include /etc/nginx/snippets/ferag.conf;

sudo nginx -t && sudo systemctl reload nginx
```

Конфиг `nginx-location-ferag.conf` уже присутствует в репозитории и содержит все нужные location-блоки (`/ferag/api/`, `/ferag/ws/`, `/ferag/`).

**Исполнение.** На cr-ubu нет `/opt/ferag`; конфиг загружен с nb-win: `scp deploy/cr-ubu/nginx-location-ferag.conf user1@176.108.244.252:~/`. Под user1 выполнены: `sudo cp ~/nginx-location-ferag.conf /etc/nginx/snippets/ferag.conf`; в `/etc/nginx/sites-available/ontoline.ru` после строки `listen 443 ssl http2;` добавлена строка `include /etc/nginx/snippets/ferag.conf;` (через `sed -i '/listen 443 ssl/a \    include ...'`); `sudo nginx -t && sudo systemctl reload nginx` — успешно. Проверка: https://ontoline.ru/ferag/ открывается, отображается страница входа (Vue SPA).

---

### ✅ 10. Порядок первого сквозного запуска

```
1. nb-win:  docker compose up -d postgres fuseki worker
2. Windows: LM Studio, модель Llama 3.3 70B, сервер :1234
3. cr-ubu:  docker compose up -d redis
4. cr-ubu:  docker compose up -d --build ferag
5. cr-ubu:  nginx -t && systemctl reload nginx
6. Открыть https://ontoline.ru/ferag/ в браузере
```

**Исполнение.** Стек поднят по шагам 1–3; п. 4 выполнен в варианте B (образ собран на nb-win, загружен на cr-ubu, контейнер ferag запущен через `docker run` в ~/ferag-deploy). П. 5–6 выполнены: nginx перезагружен, https://ontoline.ru/ferag/ открывается, отображается страница входа.

---

### ✅ 11. Финальная проверка (браузер)

Сценарий через `https://ontoline.ru/ferag/`:

1. Открыть `/ferag/` → перенаправление на `/ferag/login`.
2. Зарегистрироваться → войти.
3. Создать RAG «Тестовый».
4. Загрузить `scripts/test_source.txt` → появляется прогресс-бар шагов pipeline.
5. Дождаться `status: done` (кнопка «Подтвердить» активируется).
6. Нажать «Подтвердить» (approve).
7. Перейти во вкладку «Диалог», задать вопрос «Кто такой Alice Smith?».
8. Получить ответ LLM (не пустой, упоминает ACME Corporation).

**Исполнение (2026-02-20).** Все 8 шагов пройдены успешно.

Зафиксированные проблемы и решения:

1. **500 при регистрации** — `DATABASE_URL` в `.env` на cr-ubu указывал на `localhost:45432` вместо `10.7.0.3:45432`. Исправлено через `sed` + перезапуск контейнера ferag.
2. **500 при создании RAG** — `FUSEKI_URL` указывал на `localhost:43030`. Исправлено аналогично.
3. **graphrag: No .txt matches** — worker запускался со старым кодом (до правок `_prepare_work_dir`). Исправлено перезапуском `ferag-worker`.
4. **401 после перезахода** — `UploadView` не восстанавливал состояние. Исправлено: добавлен endpoint `GET /rags/{id}/upload-status` + `onMounted` в Vue.
5. **500 при approve (405 от Fuseki)** — датасет `ferag-00026` не существовал; Fuseki на nb-win не был доступен через WireGuard (bind `127.0.0.1` вместо `0.0.0.0`). Исправлено: датасеты созданы вручную, порт Fuseki перебиндирован на `0.0.0.0:43030:3030`, обработка 405 добавлена в `fuseki_client.py`.
6. **503 в чате (No module named 'rag_context')** — `rag_context.py` и `rag_llm.py` не входили в Docker-образ backend. Исправлено: файлы скопированы в `code/backend/`, образ пересобран и задеплоен; добавлены `LLM_API_URL=http://10.7.0.3:1234/v1` и `LLM_MODEL=llama-3.3-70b-instruct` в `.env` на cr-ubu.

Итог: ответ «Alice Smith — это инженер-программист, работающий в корпорации ACME» получен корректно. Все критерии успеха Чата 3 выполнены.

Подробно: [deploy/DEPLOYMENT_SUMMARY.md](../../deploy/DEPLOYMENT_SUMMARY.md).

---

## Критерии успеха (Чат 3)

- `https://ontoline.ru/ferag/` открывается в браузере, отображается форма входа.
- Сценарий register → login → create RAG → upload → (прогресс WebSocket) → approve → chat работает через браузер.
- `GET /rags/{id}/members` возвращает список; `POST` добавляет участника по email.
- Backend запущен в Docker-контейнере на cr-ubu (`docker ps` показывает `ferag`).
- Nginx перезагружен без ошибок; `nginx -t` → ok.

---

## Вне объёма данного плана

- AGE/pgvector-проекции и индексы (`CREATE EXTENSION age, vector`) — Чат 4 или отдельный план.
- CI/CD (GitHub Actions) — задача 9.10, при необходимости.
- История диалога (сохранение вопросов/ответов в БД) — будущий план.
- Аватары пользователей, расширенный профиль — будущий план.
- Мониторинг и метрики (Prometheus/Grafana) — будущий план.

---

## Ссылки

- Архитектура: [docs/project/PROJECT-004.md](../project/PROJECT-004.md)
- Задачи: [docs/tasks/TASKS.md](../tasks/TASKS.md) (Блок 9, задачи 9.2, 9.5–9.9)
- Nginx-конфиг: [deploy/cr-ubu/nginx-location-ferag.conf](../../deploy/cr-ubu/nginx-location-ferag.conf)
- Docker cr-ubu: [deploy/cr-ubu/docker-compose.yml](../../deploy/cr-ubu/docker-compose.yml)
- Docker nb-win: [deploy/nb-win/docker-compose.yml](../../deploy/nb-win/docker-compose.yml)
- Предшественник: [26-0219-1110_plan.md](26-0219-1110_plan.md)
- Резюме Чата 2: [26-0219-1110_plan_resume.md](26-0219-1110_plan_resume.md)
