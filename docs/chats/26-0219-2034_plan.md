# Чат 3 — Frontend (Vue) + деплой

**Дата:** 2026-02-19 (план подготовлен в конце Чата 2)  
**Предшественник:** [26-0219-1110_plan.md](26-0219-1110_plan.md) (backend + worker + полный цикл — все критерии выполнены).

---

## Исходное состояние

- **Backend**: FastAPI; все эндпоинты работают (auth, rags, upload, WebSocket, approve, chat, tasks); запускается вручную (`uvicorn`) из `code/backend/`.
- **Worker**: Celery; запущен в Docker на nb-win; цепочка задач (`run_graphrag` → `schema` → `merge` → `staging`) работает.
- **Redis**: запущен на cr-ubu (`ferag-redis`, `10.7.0.1:47379`).
- **PostgreSQL + Fuseki**: запущены в Docker на nb-win.
- **code/backend/Dockerfile**: отсутствует.
- **code/frontend/**: отсутствует.
- **cr-ubu**: только Redis-контейнер; backend не задеплоен; Nginx не настроен под ferag.
- **`/var/www/ferag/`** на cr-ubu: не создан.

---

## Задача плана

1. Добавить API управления участниками RAG (`/rags/{id}/members`).
2. Написать `code/backend/Dockerfile` для деплоя FastAPI на cr-ubu.
3. Решить задачу совместного доступа к `work_dir` между backend (cr-ubu) и worker (nb-win).
4. Реализовать `code/frontend/` — Vue 3 + Vite SPA.
5. Задеплоить backend в Docker на cr-ubu.
6. Собрать и доставить frontend на cr-ubu (`/var/www/ferag/`).
7. Подключить Nginx на cr-ubu.
8. Финальная проверка: `https://ontoline.ru/ferag/` → register → create RAG → upload → LLM-ответ.

---

## Исходное состояние (файлы и сервисы)

```
ferag/
├── code/
│   ├── backend/           # ✅ Готово (Чаты 1–2)
│   ├── worker/            # ✅ Готово (Чат 2)
│   └── frontend/          # ❌ Не существует
└── deploy/
    ├── cr-ubu/
    │   ├── docker-compose.yml   # ✅ redis + ferag; ferag не собран
    │   └── nginx-location-ferag.conf  # ✅ конфиг готов; не подключён
    └── nb-win/
        └── docker-compose.yml   # ✅ postgres + fuseki + worker
```

**Сервисы перед началом:**

| Сервис | Статус | Адрес |
|--------|--------|-------|
| PostgreSQL | ✅ запущен | `localhost:45432` / `10.7.0.3:45432` |
| Fuseki | ✅ запущен | `localhost:43030` |
| Redis | ✅ запущен (cr-ubu) | `10.7.0.1:47379` |
| Worker | ✅ запущен (Docker, nb-win) | — |
| Backend | ⚡ запускается вручную | `localhost:47821` |
| ferag-контейнер (cr-ubu) | ❌ не собран | — |
| Nginx `/ferag/` | ❌ не подключён | — |
| LM Studio | ⚡ по необходимости | `localhost:1234` |

---

## Архитектурное решение: work_dir при кросс-машинном деплое

**Проблема:** backend (cr-ubu) сохраняет загруженный файл в `work_dir` (`/tmp/ferag/rag_N/cycle_M/input/source.txt`). Worker (nb-win) читает его оттуда же. В текущем виде они на разных машинах — volume-маппинг не работает.

**Принятое решение: хранение тела файла в PostgreSQL.**

В таблицу `upload_cycles` добавляется колонка `source_content TEXT` (или `BYTEA`). При upload backend сохраняет содержимое в БД. Worker в начале `run_graphrag` читает `source_content` из БД и записывает в локальный work_dir.

**Преимущества:** не требует общего файлового хранилища; работает через существующее WireGuard-соединение backend↔PostgreSQL; простая реализация.

---

## Шаги

### 1. API управления участниками RAG

Таблица `rag_members` уже существует в схеме (`id, rag_id, user_id, role`). Нужно добавить эндпоинты в `code/backend/app/routers/rags.py`.

#### 1.1 `POST /rags/{rag_id}/members`

```python
class MemberAddBody(BaseModel):
    email: str
    role: str  # 'viewer' | 'editor'

@router.post("/{rag_id}/members", status_code=201)
def add_member(rag_id: int, body: MemberAddBody, ...):
    # Только owner может добавлять участников
    # Найти User по email (404 если нет)
    # Проверить: уже не owner и не member
    # Создать RagMember(rag_id, user_id=target.id, role=body.role)
    # Вернуть {user_id, email, role}
```

#### 1.2 `GET /rags/{rag_id}/members`

```python
@router.get("/{rag_id}/members")
def list_members(rag_id: int, ...):
    # Вернуть список участников (user_id, email, display_name, role)
    # Доступен owner и любой member
```

#### 1.3 `DELETE /rags/{rag_id}/members/{user_id}`

```python
@router.delete("/{rag_id}/members/{user_id}", status_code=204)
def remove_member(rag_id: int, user_id: int, ...):
    # Только owner; нельзя удалить самого себя (он owner)
    # Удалить запись RagMember
```

---

### 2. Alembic: миграция `source_content`

Добавить поле в модель `UploadCycle`:

```python
source_content: Mapped[Optional[str]] = mapped_column(Text, nullable=True)
```

Сгенерировать и применить миграцию:

```bash
cd code/backend
alembic revision --autogenerate -m "add_upload_cycle_source_content"
alembic upgrade head
```

---

### 3. Backend: адаптация upload для кросс-машинного деплоя

#### 3.1 Сохранение содержимого в БД

В `POST /rags/{id}/upload` добавить запись содержимого файла в `UploadCycle.source_content` (вместо/вместе с записью на диск):

```python
content = await file.read()
cycle.source_content = content.decode("utf-8")
db.commit()
```

Локальная запись на диск остаётся для обратной совместимости при dev-запуске.

#### 3.2 Worker: чтение из БД

В `tasks/graphrag_task.py` в `_prepare_work_dir`:

```python
# Если файл не существует локально — читаем из БД
if not input_path.exists():
    with get_db_session() as db:
        cycle = db.get(UploadCycle, cycle_id)
        if cycle and cycle.source_content:
            input_path.parent.mkdir(parents=True, exist_ok=True)
            input_path.write_text(cycle.source_content)
        else:
            raise FileNotFoundError(f"source not found for cycle {cycle_id}")
```

---

### 4. `code/backend/Dockerfile`

```dockerfile
FROM python:3.11-slim

RUN apt-get update && apt-get install -y build-essential curl \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . /app

# Worker пакет нужен для отправки задач в Celery (start_update_chain)
# Добавляется через volume в docker-compose (CELERY_TASKS_ONLY=1)
# или через COPY ниже для standalone-деплоя:
# COPY ../worker /app/worker_pkg
# ENV PYTHONPATH=/app:/app/worker_pkg

ENV PYTHONPATH=/app

EXPOSE 47821
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "47821"]
```

**Зависимость от worker-пакета:** при отправке Celery-задач backend импортирует `from worker.tasks import start_update_chain`. На cr-ubu этот пакет недоступен без явной копии. Решение: в `deploy/cr-ubu/docker-compose.yml` для сервиса `ferag` добавить volume, монтирующий `code/worker` в контейнер как `/app/worker`, и обновить PYTHONPATH.

Альтернативное решение (более чистое): заменить прямой импорт на `celery.send_task` по имени строки, избавившись от зависимости на файловую систему worker-пакета.

**Рекомендованный путь:** использовать `send_task` по имени.

В `code/backend/app/routers/rags.py` заменить:

```python
from worker.tasks import start_update_chain
start_update_chain(rag_id, cycle.id, task.id, str(file_path))
```

На:

```python
from app.celery_sender import send_update_chain
send_update_chain(rag_id, cycle.id, task.id, str(file_path))
```

Создать `code/backend/app/celery_sender.py`:

```python
from celery import Celery
from app.config import get_settings

def _get_celery() -> Celery:
    s = get_settings()
    return Celery(broker=s.celery_broker_url, backend=s.celery_result_backend)

def send_update_chain(rag_id, cycle_id, task_id, input_file):
    app = _get_celery()
    from celery import chain, signature
    chain(
        app.signature("worker.tasks.graphrag_task.run_graphrag",
                      args=[rag_id, cycle_id, task_id, input_file]),
        app.signature("worker.tasks.schema_task.run_schema_induction",
                      args=[rag_id, cycle_id, task_id], immutable=True),
        app.signature("worker.tasks.merge_task.do_merge",
                      args=[rag_id, cycle_id, task_id], immutable=True),
        app.signature("worker.tasks.staging_task.load_to_staging",
                      args=[rag_id, cycle_id, task_id], immutable=True),
    ).apply_async()
```

---

### 5. `deploy/cr-ubu/docker-compose.yml`: ferag-контейнер

Файл уже содержит сервис `ferag`; нужно убедиться в корректности:

```yaml
ferag:
  build:
    context: ../../code/backend
    dockerfile: Dockerfile
  container_name: ferag
  restart: unless-stopped
  ports:
    - "127.0.0.1:47821:47821"
  environment:
    - DATABASE_URL=postgresql://ferag:${DB_PASSWORD}@10.7.0.3:45432/ferag_app
    - REDIS_URL=redis://redis:47379/0
    - CELERY_BROKER_URL=redis://redis:47379/0
    - CELERY_RESULT_BACKEND=redis://redis:47379/1
    - JWT_SECRET=${JWT_SECRET}
    - FUSEKI_URL=http://10.7.0.3:43030
    - FUSEKI_USER=admin
    - FUSEKI_PASSWORD=${FUSEKI_PASSWORD}
    - ALLOWED_ORIGINS=https://ontoline.ru
  depends_on:
    - redis
  networks:
    - ferag-network
```

**Важно:** Fuseki на nb-win сейчас слушает только на `127.0.0.1:43030`. Для доступа с cr-ubu нужно также открыть `10.7.0.3:43030` в `deploy/nb-win/docker-compose.yml`:

```yaml
ports:
  - "127.0.0.1:43030:3030"   # localhost (worker внутри Docker-сети)
  - "10.7.0.3:43030:3030"    # WireGuard → backend на cr-ubu
```

---

### 6. `code/frontend/` — Vue 3 + Vite

#### 6.1 Инициализация проекта

```bash
cd code
npm create vue@latest frontend
# Options: TypeScript ✅, Vue Router ✅, Pinia ✅, ESLint ✅
cd frontend
npm install axios
```

`vite.config.ts`:

```typescript
import { defineConfig } from 'vite'
import vue from '@vitejs/plugin-vue'

export default defineConfig({
  plugins: [vue()],
  base: '/ferag/',
  server: {
    proxy: {
      '/ferag/api': { target: 'http://localhost:47821', rewrite: p => p.replace('/ferag/api', '') },
      '/ferag/ws': { target: 'ws://localhost:47821', ws: true, rewrite: p => p.replace('/ferag/ws', '/ws') },
    }
  }
})
```

#### 6.2 Структура

```
code/frontend/src/
├── api/
│   ├── client.ts       # axios instance, baseURL='/ferag/api/', JWT interceptor
│   ├── auth.ts         # register, login, me
│   ├── rags.ts         # listRags, createRag, getRag, deleteRag
│   ├── upload.ts       # uploadFile → {cycle_id, task_id}
│   ├── tasks.ts        # getTask
│   ├── approve.ts      # approveCycle
│   ├── chat.ts         # sendQuestion → {answer, context_used}
│   └── members.ts      # listMembers, addMember, removeMember
├── stores/
│   ├── auth.ts         # useAuthStore: token, user, login(), logout()
│   └── rags.ts         # useRagsStore: list, current
├── router/
│   └── index.ts        # маршруты (guards: requireAuth)
├── views/
│   ├── LoginView.vue       # /ferag/login — форма входа/регистрации
│   ├── RagsView.vue        # /ferag/ — список RAG
│   ├── RagDetailView.vue   # /ferag/rags/:id — карточка RAG + вкладки
│   ├── UploadView.vue      # /ferag/rags/:id/upload — загрузка + прогресс
│   ├── ChatView.vue        # /ferag/rags/:id/chat — диалог
│   └── MembersView.vue     # /ferag/rags/:id/members — участники
└── components/
    ├── NavBar.vue
    ├── TaskProgress.vue    # WebSocket-прогресс задачи (шаги pipeline)
    └── MessageBubble.vue   # пузырь вопрос/ответ
```

#### 6.3 WebSocket в `TaskProgress.vue`

```typescript
const ws = new WebSocket(
  `wss://ontoline.ru/ferag/ws/tasks/${taskId}?token=${authStore.token}`
)
ws.onmessage = (e) => {
  const msg = JSON.parse(e.data)   // {status, step, error}
  steps.value.push(msg)
  if (msg.status === 'done') emit('done')
  if (msg.status === 'failed') emit('failed', msg.error)
}
```

При dev-запуске: `ws://localhost:47821/ws/tasks/{id}?token=…`.

#### 6.4 Сборка

```bash
cd code/frontend
npm run build   # → dist/
```

Артефакты: `code/frontend/dist/` (index.html + assets/). Vite вставляет `base='/ferag/'` в пути ресурсов.

---

### 7. Деплой frontend на cr-ubu

```bash
# На nb-win (WSL2)
rsync -avz code/frontend/dist/ user@cr-ubu:/var/www/ferag/
# или через SCP:
scp -r code/frontend/dist/* user@cr-ubu:/var/www/ferag/
```

На cr-ubu:

```bash
sudo mkdir -p /var/www/ferag
sudo chown www-data:www-data /var/www/ferag
```

---

### 8. Деплой backend-контейнера на cr-ubu

На cr-ubu необходим доступ к исходному коду (`code/backend/`). Варианты:

**Вариант A (рекомендован для lab):** клонировать репозиторий на cr-ubu, собирать образ там.

```bash
# На cr-ubu
git clone <repo> /opt/ferag
cd /opt/ferag/deploy/cr-ubu
cp .env.example .env  # заполнить секреты
docker compose up -d --build ferag
```

**Вариант B:** собирать образ на nb-win, экспортировать через `docker save`, заливать на cr-ubu.

```bash
# nb-win
docker build -t ferag-backend:latest code/backend/
docker save ferag-backend:latest | gzip > ferag-backend.tar.gz
scp ferag-backend.tar.gz cr-ubu:/tmp/
# cr-ubu
docker load < /tmp/ferag-backend.tar.gz
docker compose up -d ferag
```

---

### 9. Nginx на cr-ubu

```bash
# На cr-ubu
sudo cp /opt/ferag/deploy/cr-ubu/nginx-location-ferag.conf \
    /etc/nginx/snippets/ferag.conf

# В /etc/nginx/sites-available/ontoline.ru внутри server { listen 443 ssl ... }
# добавить строку:
#     include /etc/nginx/snippets/ferag.conf;

sudo nginx -t && sudo systemctl reload nginx
```

Конфиг `nginx-location-ferag.conf` уже присутствует в репозитории и содержит все нужные location-блоки (`/ferag/api/`, `/ferag/ws/`, `/ferag/`).

---

### 10. Порядок первого сквозного запуска

```
1. nb-win:  docker compose up -d postgres fuseki worker
2. Windows: LM Studio, модель Llama 3.3 70B, сервер :1234
3. cr-ubu:  docker compose up -d redis
4. cr-ubu:  docker compose up -d --build ferag
5. cr-ubu:  nginx -t && systemctl reload nginx
6. Открыть https://ontoline.ru/ferag/ в браузере
```

---

### 11. Финальная проверка (браузер)

Сценарий через `https://ontoline.ru/ferag/`:

1. Открыть `/ferag/` → перенаправление на `/ferag/login`.
2. Зарегистрироваться → войти.
3. Создать RAG «Тестовый».
4. Загрузить `scripts/test_source.txt` → появляется прогресс-бар шагов pipeline.
5. Дождаться `status: done` (кнопка «Подтвердить» активируется).
6. Нажать «Подтвердить» (approve).
7. Перейти во вкладку «Диалог», задать вопрос «Кто такой Alice Smith?».
8. Получить ответ LLM (не пустой, упоминает ACME Corporation).

---

## Критерии успеха (Чат 3)

- `https://ontoline.ru/ferag/` открывается в браузере, отображается форма входа.
- Сценарий register → login → create RAG → upload → (прогресс WebSocket) → approve → chat работает через браузер.
- `GET /rags/{id}/members` возвращает список; `POST` добавляет участника по email.
- Backend запущен в Docker-контейнере на cr-ubu (`docker ps` показывает `ferag`).
- Nginx перезагружен без ошибок; `nginx -t` → ok.

---

## Вне объёма данного плана

- AGE/pgvector-проекции и индексы (`CREATE EXTENSION age, vector`) — Чат 4 или отдельный план.
- CI/CD (GitHub Actions) — задача 9.10, при необходимости.
- История диалога (сохранение вопросов/ответов в БД) — будущий план.
- Аватары пользователей, расширенный профиль — будущий план.
- Мониторинг и метрики (Prometheus/Grafana) — будущий план.

---

## Ссылки

- Архитектура: [docs/project/PROJECT-004.md](../project/PROJECT-004.md)
- Задачи: [docs/tasks/TASKS.md](../tasks/TASKS.md) (Блок 9, задачи 9.2, 9.5–9.9)
- Nginx-конфиг: [deploy/cr-ubu/nginx-location-ferag.conf](../../deploy/cr-ubu/nginx-location-ferag.conf)
- Docker cr-ubu: [deploy/cr-ubu/docker-compose.yml](../../deploy/cr-ubu/docker-compose.yml)
- Docker nb-win: [deploy/nb-win/docker-compose.yml](../../deploy/nb-win/docker-compose.yml)
- Предшественник: [26-0219-1110_plan.md](26-0219-1110_plan.md)
- Резюме Чата 2: [26-0219-1110_plan_resume.md](26-0219-1110_plan_resume.md)
