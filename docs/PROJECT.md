# Проектная документация: ferag

## 1. Назначение и философская основа проекта

**ferag** — лабораторный проект по построению локальной системы графового RAG (Retrieval-Augmented Generation) на основе текста. Цель: превращать текстовые корпуса в структурированный граф знаний (онтологию) и получать ответы на сложные вопросы, опираясь на связи между сущностями.

### 1.1. Диалектический подход: Тезис — Антитезис — Синтез

Проект основан на классической диалектической методологии познания, применённой к построению графа знаний:

#### **Тезис** (Новое знание)
- **Субъект**: Локальная LLM (Ollama + Llama 3.2/3.3, DeepSeek-R1)
- **Объект**: Новый корпус текста
- **Действие**: Извлечение сущностей и связей, формирование триплетов (субъект-предикат-объект)
- **Результат**: Неорганизованный массив триплетов — сырые данные без структуры
- **Характер**: Хаос, потенциальное знание, требующее упорядочивания

#### **Антитезис** (Существующее знание)
- **Субъект**: Ранее синтезированная онтология
- **Природа**: Структурированная система классов сущностей и типов связей
- **Действие**: Предоставление формальной схемы для классификации триплетов
- **Характер**: Порядок, организованное накопленное знание

#### **Синтез** (Разрешение противоречия)

Когда неорганизованные триплеты (Тезис) сталкиваются с существующей онтологией (Антитезис), происходит процесс синтеза нового знания. Массив триплетов разделяется на две части:

**А) "Горизонтальный/Ламинарный/Эволюционный" рост:**
- Триплеты, которые "ложатся" в существующую структуру онтологии
- Дополняют имеющиеся классы и отношения новыми экземплярами
- Онтология количественно растёт, но структурно остаётся стабильной
- Пример: новый `Person → worksFor → Organization`, где типы уже определены

**Б) "Вертикальный/Турбулентный/Революционный" рост:**
- Триплеты, которые не вписываются в существующую схему
- Требуют расширения онтологии: новые классы, новые типы отношений, модификация иерархии
- Онтология претерпевает структурные изменения
- Пример: обнаружение нового типа связи между сущностями, отсутствующего в схеме

**Критерий завершения синтеза:**
Онтология включает весь массив триплетов (или его часть, определённую пользователем). Противоречие между хаосом новых данных и порядком существующей структуры разрешено.

**Важно:** Новая обогащённая онтология сама становится **Антитезисом** для следующего корпуса текста (нового Тезиса) → циклический процесс познания.

### 1.2. Эксплуатация vs. Синтез

**Эксплуатация** — это получение ответов на вопросы пользователя по уже синтезированной онтологии. Это использование готового знания, а не создание нового. Эксплуатация следует после синтеза и не является его частью.

---

## 2. Цели и результаты

| Цель | Описание |
|------|----------|
| Локальный запуск LLM | Запуск больших языковых моделей на своём ПК без облачных API. |
| Графовый RAG | Построение графа знаний из текста и ответы на сложные/глобальные вопросы. |
| Онтология как ядро | Создание и развитие онтологии, которая структурирует извлечённые из текста триплеты. |
| Диалектический цикл | Реализация циклического процесса познания: новые данные → синтез → обогащённая онтология. |
| Визуализация | Инструменты для визуализации графа знаний и онтологий (Neo4j, Protégé, QGIS). |
| Воспроизводимая среда | Windows + WSL2 + Cursor + Ollama + Docker для разработки и экспериментов. |

---

## 3. Целевая платформа (железо)

### 3.1. Текущая конфигурация

| Параметр | Значение |
|----------|----------|
| Устройство | Мини-ПК GMKtec NucBox K8 Plus (Barebone) |
| Процессор | AMD Ryzen 7 8845HS (Zen 4, NPU Ryzen AI, 8 ядер) |
| ОЗУ | 64 ГБ |
| iGPU | AMD Radeon 780M (выделено 8 ГБ из ОЗУ в BIOS) |
| Хранилище (основной) | Samsung 990 Pro 2 ТБ (C: 300 ГБ Windows, D: ~1.7 ТБ данные) |
| Хранилище (планируется) | Второй SSD M.2 NVMe 1-2 ТБ для Fedora 43 |
| Порты расширения | OCuLink (для будущего подключения eGPU) |
| Сеть | AMD RZ616 Wi-Fi 6E 160MHz (MediaTek MT7922), Realtek 2.5G Ethernet ×2 |

### 3.2. Ожидаемые возможности (на 2026)

**Текущая конфигурация (iGPU):**
- LLM: модели 7B–14B комфортно; 30B-70B с квантованием (Q4_K_M, Q5_K_M)
- LightRAG: 50–100 тыс. страниц (200–500 МБ текста); индексация 100 стр. ~20–30 мин
- MS GraphRAG: до 5–10 тыс. страниц; индексация существенно медленнее, но глубже по аналитике

**С eGPU через OCuLink (перспектива):**
- LLM: модели до 120B с высоким квантованием (Q6, Q8)
- Скорость индексации: ускорение в 3-5 раз
- Параллельная работа: 2-3 модели одновременно
- Видеокарты: RTX 4070 Ti / 4080 (16 ГБ VRAM)

---

## 4. Целевой программный стек

### 4.1. Основные компоненты

| Компонент | Назначение |
|-----------|------------|
| **Windows 11 Pro** | Хост-ОС, визуализация, GUI-инструменты, Ollama, Cursor, Docker Desktop. |
| **WSL2 (Ubuntu)** | Разработка (проект `ferag`), Python, Docker-контейнеры, вычисления. |
| **Fedora 43** (планируется) | Нативная Linux-среда для тяжёлых вычислений, индексации больших корпусов, моделей 70B+. |
| **Cursor** | IDE (форк VS Code), интеграция с WSL, связь с Ollama (http://localhost:11434) для автодополнения. Cursor Agent CLI для помощи в терминале. |
| **Ollama** | Локальный запуск LLM (Llama 3.3 70B Q4_K_M, DeepSeek-R1) для извлечения триплетов, Schema Induction и диалога с пользователем. |
| **Docker Desktop** | Запуск БД (PostgreSQL, MillenniumDB) и сервисов RAG в контейнерах. |
| **MS GraphRAG** | Фреймворк для построения графа знаний с community detection и entity resolution. Основной инструмент извлечения триплетов. |
| **PostgreSQL 16** | Реляционная СУБД как основа multi-model решения (граф + реляционная + векторный поиск). |
| **Apache AGE** | Графовое расширение PostgreSQL (Property Graph, Cypher). Рабочий кэш для быстрых запросов. |
| **pgvector** | Расширение PostgreSQL для векторного поиска (эмбеддинги, семантический поиск для RAG). |
| **Apache Jena Fuseki** | RDF/SPARQL 1.1 сервер. Источник истины для онтологии (OWL/RDF) с полной поддержкой OWL reasoning и RDFS inference. |
| **LlamaIndex** | Библиотека для RAG и Schema Induction (извлечение онтологии из триплетов). |
| **LangChain** | Библиотека для построения RAG-пайплайнов и диалога с пользователем. |

### 4.2. Инструменты визуализации и работы с онтологиями

| Инструмент | Назначение | Платформа |
|------------|------------|-----------|
| **WebProtégé** | Веб-редактор онтологий (OWL, RDF), collaborative, 68K+ проектов. Рекомендуется вместо desktop Protégé. | Веб (https://webprotege.stanford.edu) |
| **Protégé Desktop** | Десктопный редактор онтологий (OWL, RDF), визуализация классов и отношений. Опциональная альтернатива WebProtégé. | Windows |
| **pgAdmin** | Веб-интерфейс для управления PostgreSQL, визуализация данных, выполнение SQL/Cypher запросов | Windows/WSL |
| **Apache AGE Viewer** | Веб-интерфейс для визуализации графов Apache AGE (опционально) | Windows/WSL |
| **QGIS** | Работа с картографическими данными, геопривязка сущностей из графа | Windows |

---

## 5. Архитектура и диалектический цикл

### 5.1. Три элемента системы (концепция "прокачивания информации")

Система в любой момент состоит из трёх элементов:

1. **Сырые тексты** (элемент 1) — файловая система `data/raw/`
2. **Неорганизованные триплеты** (элемент 2) — staging DB (MillenniumDB: `<staging_triplets>`)
3. **Сбалансированные триплеты + онтология** (элемент 3) — production DB (MillenniumDB: `<ontology_graph>` + `<knowledge_graph>`)

Процесс развития: **"прокачивание" информации** от элемента 1 к элементу 3 → эксплуатация → новая информация → цикл повторяется.

---

### 5.2. Полный диалектический цикл

```
┌──────────────────────────────────────────────┐
│  ЭТАП 1: ТЕЗИС (Получение неорганизованных   │
│  триплетов)                                  │
└──────────────────────────────────────────────┘

1. Сырые тексты → data/raw/
2. MS GraphRAG индексация:
   • Ollama (Llama 3.3 70B Q4_K_M, 40 ГБ)
   • Извлечение сущностей и связей
   • Entity resolution (объединение дубликатов)
   • Community detection (кластеры связанных сущностей)
   • Hierarchical communities (иерархия сообществ)
   • Summarization (описание каждого сообщества)
   Время: 2-4 часа на 100 документов
3. Результат:
   • RDF триплеты (высокое качество, без дубликатов)
   • Communities (метаданные)
   • Summaries (контекст)
4. Загрузка в MillenniumDB:
   • Граф <staging_triplets> (триплеты)
   • Граф <staging_communities> (метаданные)

           ↓

┌──────────────────────────────────────────────┐
│  ЭТАП 2: АНТИТЕЗИС (Извлечение онтологии из  │
│  триплетов) — Schema Induction               │
└──────────────────────────────────────────────┘

5. Knowledge Graph Schema Induction:
   • LlamaIndex SchemaExtractor или кастомный скрипт
   • Ollama (Llama 3.3 70B Q4_K_M)
   • Вход: триплеты + communities + summaries
   • Анализ: LLM обобщает структуру данных
   Время: 2-4 часа на 10,000 триплетов
6. Результат:
   • OWL/RDF онтология (классы, свойства, иерархии)
   • Классы ← на основе communities
   • Иерархия классов ← на основе hierarchical communities
   • Domain/Range ← на основе анализа связей
7. Сохранение:
   • MillenniumDB: граф <new_ontology>
   • Или файл: ontologies/extracted_ontology.owl

           ↓

┌──────────────────────────────────────────────┐
│  ЭТАП 3: СИНТЕЗ (Интеграция и балансирование)│
└──────────────────────────────────────────────┘

8. Сравнение онтологий (SPARQL):
   • Извлечённая онтология (<new_ontology>)
   • Существующая онтология (<ontology_graph>)
   • Поиск различий (новые классы, свойства)
9. Классификация триплетов:
   • Ламинарные — вписываются в существующую онтологию
   • Турбулентные — требуют изменения онтологии
10. Очистка аномалий (Graph Validation):
    • SPARQL-запросы для поиска нарушений domain/range
    • Удаление или пометка для ручной проверки
11. Синтез интегрированной онтологии:
    • Объединение: existing + new + правки для турбулентных
    • Результат: обогащённая онтология
12. Обновление production DB:
    • MillenniumDB <ontology_graph> ← интегрированная онтология
    • MillenniumDB <knowledge_graph> ← все триплеты (старые + новые)

           ↓

┌──────────────────────────────────────────────┐
│  ЭТАП 4: ПРОЕКЦИИ (Ветвление представлений)  │
└──────────────────────────────────────────────┘

13. Синхронизация RDF → Property Graph:
    • MillenniumDB (RDF) → PostgreSQL + AGE (Property Graph)
    • Конвертация: RDF триплеты → Cypher (nodes, edges)
    • Apache AGE = рабочий кэш для быстрых запросов
14. Векторизация документов:
    • Документы → эмбеддинги → pgvector
    • Для семантического поиска в RAG
15. Реляционные проекции:
    • Apache AGE → Materialized Views (PostgreSQL)
    • Агрегация для аналитики (SQL)

           ↓

┌──────────────────────────────────────────────┐
│  ЭТАП 5: ЭКСПЛУАТАЦИЯ (Диалог с пользователем)│
└──────────────────────────────────────────────┘

16. RAG (Retrieval-Augmented Generation):
    • Вопрос пользователя
    • Hybrid Retriever:
      - Векторный поиск (pgvector) — похожие документы
      - Граф-обход (Apache AGE Cypher) — связанные сущности
      - Реляционная аналитика (SQL) — агрегация
    • Контекст → LLM (Ollama) → Ответ
17. Обновление знаний:
    • Новая информация из диалога → data/raw/
    • Накопление 1000+ новых фактов → повтор цикла (ЭТАП 1)
```

---

### 5.3. Иерархия организации знаний (уточнённая)

```
Текст (хаос, максимальная энтропия)
  ↓
Триплеты без онтологии (частичный порядок)
  ↓
RDF/SPARQL + OWL онтология (Apache Jena Fuseki + reasoning)
  ┃
  ┃ ← ВЕРШИНА СЕМАНТИЧЕСКОЙ ОРГАНИЗАЦИИ
  ┃    (Источник истины: формальная семантика, OWL/RDFS inference)
  ┃
  ┗━━━ Ветвление на специализированные представления:
      │
      ├─ Property Graph (PostgreSQL + Apache AGE)
      │  └─ Проекция RDF для быстрых Cypher-запросов
      │
      ├─ Реляционные таблицы (PostgreSQL Materialized Views)
      │  └─ Проекция графа для SQL-агрегации и отчётов
      │
      ├─ JSON документы (PostgreSQL JSONB)
      │  └─ Проекция для API и гибкой схемы
      │
      └─ Векторные индексы (pgvector)
         └─ Проекция для семантического поиска
```

**Ключевой принцип:** 
- **Apache Jena Fuseki (RDF/OWL + reasoning)** — источник истины (single source of truth)
- **PostgreSQL + AGE** — рабочий кэш для производительности
- **Materialized Views** — представления для аналитики

---

### 5.4. Docker и базы данных

**Данные Docker:** При движке WSL 2 образы и тома хранятся в виртуальном диске WSL (файл `ext4.vhdx` на диске D:).

**Проверка:** Из терминала Cursor (WSL) команда `docker ps` должна выполняться без ошибок. Если демон недоступен — запустить Docker Desktop в Windows и включить WSL Integration для Ubuntu (Settings → Resources → WSL Integration).

---

**Первый запуск БД (задача 4.4):**

#### PostgreSQL + Apache AGE + pgvector (основная БД)

**Вариант A: VectorGraph (рекомендуется)**

```bash
cd ~/projects
git clone https://github.com/QuixiAI/vectorgraph.git
cd vectorgraph
docker-compose up -d
```

Что включено: PostgreSQL 16 + Apache AGE + pgvector в одном контейнере.  
Подключение: localhost:5432, логин: postgres

**Вариант Б: Apache AGE официальный образ**

```bash
docker run -d --name postgres-age \
  -e POSTGRES_PASSWORD=ferag2026 \
  -p 5432:5432 \
  -v ~/projects/ferag/postgres-data:/var/lib/postgresql/data \
  apache/age:PG16_latest

# Установить pgvector
docker exec -it postgres-age psql -U postgres -c "CREATE EXTENSION vector;"
```

**Настройка после запуска:**

```sql
-- Подключиться: docker exec -it postgres-age psql -U postgres
CREATE EXTENSION IF NOT EXISTS age;
CREATE EXTENSION IF NOT EXISTS vector;
LOAD 'age';
SET search_path = ag_catalog, "$user", public;

-- Создать граф для знаний
SELECT create_graph('knowledge_graph');
```

---

#### Apache Jena Fuseki (источник истины для онтологии)

**Установка:**

```bash
docker run -d --name fuseki \
  -p 3030:3030 \
  -v ~/projects/ferag/fuseki-data:/fuseki \
  -e ADMIN_PASSWORD=ferag2026 \
  stain/jena-fuseki
```

**Веб-интерфейс:** http://localhost:3030  
**SPARQL endpoint:** http://localhost:3030/$/datasets  
**Поддержка:** RDF, SPARQL 1.1, OWL reasoning, RDFS inference

**Назначение:**
- Хранение онтологии (OWL/RDF-схема) с формальной семантикой
- Хранение всех триплетов (RDF triple store)
- SPARQL-запросы с логическим выводом (inference)
- Источник истины (single source of truth)

**Ключевые возможности:**
- **OWL reasoning:** OWLFBRuleReasoner, OWLMicroReasoner, OWLMiniReasoner
- **RDFS inference:** rdfs:subClassOf, rdfs:subPropertyOf, rdfs:domain, rdfs:range
- **Критично для проекта:** Schema Induction требует inference для иерархии классов, Synthesis требует owl:equivalentClass для объединения онтологий

**Создание dataset:**

```bash
# Через веб-интерфейс: http://localhost:3030 → "Manage datasets" → "New dataset"
# Или через API:
curl -X POST http://localhost:3030/$/datasets \
  -u admin:ferag2026 \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "dbName=ferag&dbType=tdb2"
```

**Загрузка RDF (Turtle формат):**

```bash
curl -X POST http://localhost:3030/ferag/data \
  -u admin:ferag2026 \
  -H "Content-Type: text/turtle" \
  --data-binary @ontology.ttl
```

**SPARQL-запрос с inference:**

```bash
curl -X POST http://localhost:3030/ferag/sparql \
  -H "Accept: application/sparql-results+json" \
  -H "Content-Type: application/sparql-query" \
  --data-binary "SELECT ?s ?p ?o WHERE { ?s ?p ?o } LIMIT 10"
```

**Пример inference:**

```sparql
# Если в онтологии есть:
:Employee rdfs:subClassOf :Person .
:Alice rdf:type :Employee .

# То Fuseki автоматически выведет:
:Alice rdf:type :Person .  # ← inference!
```

---

**Ресурсы (опционально):** Docker Desktop → Settings → Resources — ограничить Memory/CPUs, чтобы контейнеры не конкурировали с Ollama.

**WSL:** Лимит памяти задаётся в `C:\Users\Admin\.wslconfig` (memory=32GB). После изменений: PowerShell → `wsl --shutdown`.

---

## 6. Организация данных и структура проекта

### 6.1. Разделение дисков (Samsung 990 Pro 2 ТБ)

```
├── C: (Windows 11 Pro, 300 ГБ, NTFS)
│   ├── Windows/                  # ОС
│   ├── Program Files/            # Приложения (Cursor, Docker Desktop, Ollama, Protégé, QGIS)
│   ├── Users/
│   │   └── Admin/
│   │       ├── AppData/Local/
│   │       │   └── Cursor/  → D:\AI\Cursor (символическая ссылка)
│   │       └── .cursor/
│   └── [Pagefile.sys]            # Файл подкачки (фиксированный, 16 ГБ)
│
└── D: (DATA, ~1.7 ТБ, NTFS)
    ├── AI/
    │   ├── AI_MODELS/            # Веса моделей Ollama (~200-400 ГБ)
    │   │   ├── llama3.2/
    │   │   ├── llama3.3-70b/
    │   │   └── deepseek-r1/
    │   │
    │   ├── WSL/                  # Виртуальный диск WSL2
    │   │   └── Ubuntu/
    │   │       └── ext4.vhdx     # ~50-100 ГБ (растёт)
    │   │
    │   └── Cursor/               # Локальные данные Cursor
    │
    ├── projects/                 # Проекты и код
    │   └── ferag/                 # Этот проект (монтируется в WSL)
    │
    ├── ontologies/               # OWL/RDF-схемы онтологий
    ├── graphs/                   # Экспорты графовых БД
    └── backups/                  # Резервные копии
```

### 6.2. Структура проекта ferag

```
ferag/
├── docs/                 # Документация проекта
│   ├── PROJECT.md        # Этот файл — проектная документация
│   ├── TASKS.md          # Задачи и ход исполнения
│   └── JOURNAL.md        # Журнал проекта (события, чаты)
├── src/                  # Исходный код (скрипты, пайплайны RAG)
├── data/                 # Входные текстовые корпуса
├── ontologies/           # Созданные онтологии (OWL, RDF)
├── configs/              # Конфигурации LightRAG/GraphRAG
├── 26-0201_chat.txt      # Исходный чат-источник решений
└── README.md
```

### 6.3. Настройки приложений

**Ollama** (переменная окружения):
```powershell
OLLAMA_MODELS=D:\AI\AI_MODELS
```

**Docker Desktop** (Settings → Resources → Advanced):
```yaml
data-root: D:\AI\WSL\Ubuntu\docker
```

**WSL (.wslconfig)** в `C:\Users\Admin\.wslconfig`:
```ini
[wsl2]
memory=32GB              # Треть от 64 ГБ (остальное для Windows и Ollama)
processors=8             # Все ядра Ryzen 7 8845HS
localhostForwarding=true
```

---

## 7. MS GraphRAG и Schema Induction: Ядро проекта

### 7.1. Почему MS GraphRAG (а не LightRAG)

**Ключевая задача проекта:** Создание онтологии из сырых триплетов (Schema Induction) — наиболее интеллектуальный и творческий момент.

**MS GraphRAG преимущества для Schema Induction:**

1. **Community Detection** → автоматические подсказки для классов онтологии
   - Находит кластеры связанных сущностей
   - Группа "Tech Companies" → класс "Organization"
   - Группа "Employees" → класс "Person"

2. **Hierarchical Communities** → иерархия классов (rdfs:subClassOf)
   - Level 0: "All Entities"
   - Level 1: "People", "Organizations", "Events"
   - Level 2: "Employees", "Managers" (subClassOf: People)

3. **Entity Resolution** → чистые триплеты без дубликатов
   - "Alice", "Alice Smith", "A. Smith" → "Alice Smith"
   - Меньше аномалий → качественнее онтология

4. **Summarization** → контекст для LLM при Schema Induction
   - Каждое сообщество имеет описание
   - LLM лучше понимает структуру данных

**Недостатки MS GraphRAG:**
- ⚠️ Медленная индексация (2-4 часа на 100 документов)
- ❌ Нет инкрементальных обновлений (нужна переиндексация всего корпуса)

**Но:** Качество онтологии важнее скорости → MS GraphRAG оптимален для проекта.

---

### 7.2. Knowledge Graph Schema Induction

**Что это:** Автоматическое извлечение онтологии (OWL/RDF) из неорганизованных триплетов.

**Инструменты:**
- **LlamaIndex SchemaExtractor** — библиотека для извлечения схемы
- **Кастомные скрипты** с Llama 3.3 70B Q4_K_M

**Процесс:**

```python
# 1. Извлечь триплеты из staging (MillenniumDB)
staging_triplets = millenniumdb.query("SELECT ?s ?p ?o WHERE { ... }")

# 2. Извлечь communities из MS GraphRAG
communities = load_communities("data/graphrag/output/communities.parquet")

# 3. LLM анализирует триплеты + communities
prompt = f"""
Триплеты: {staging_triplets}
Communities: {communities}

Создай OWL-онтологию:
1. Классы (на основе communities)
2. Иерархия (rdfs:subClassOf на основе hierarchical communities)
3. Свойства (owl:ObjectProperty на основе типов связей)
4. Ограничения (rdfs:domain, rdfs:range)
"""

# 4. LLM генерирует онтологию (2-4 часа)
ontology_owl = llm.generate(prompt)

# 5. Сохранить в MillenniumDB
millenniumdb.insert(graph="<new_ontology>", data=ontology_owl)
```

**Производительность на GMKtec NucBox K8 Plus:**
- Llama 3.3 70B Q4_K_M (40 ГБ) — реально на 64 ГБ ОЗУ
- Скорость: 1.5–3 токена/сек
- Время: 2-4 часа на 10,000 триплетов
- **Запускать на ночь** или в фоне

---

### 7.3. Работа с онтологиями: Три подхода

#### Подход 1: Pydantic-схемы (Этап 1, для начала)

**Применение:** Простая онтология для валидации триплетов.

```python
from pydantic import BaseModel
from typing import Literal

class Entity(BaseModel):
    name: str
    type: Literal["Person", "Organization", "Event"]

class Relation(BaseModel):
    source: str
    predicate: Literal["worksFor", "participatedIn"]
    target: str
```

**Преимущества:** Быстро начать, валидация "из коробки".  
**Недостатки:** Не настоящая OWL-онтология, нет inference.

---

#### Подход 2: WebProtégé + Schema Induction (Этап 2, рекомендуется)

**Применение:** Формальная OWL-онтология с визуализацией.

**Процесс:**
1. Создать проект в WebProtégé (https://webprotege.stanford.edu/)
2. Построить базовую онтологию (классы, иерархии)
3. MS GraphRAG → триплеты → Schema Induction → обогащённая онтология
4. Импорт обратно в WebProtégé для визуализации/редактирования
5. Экспорт в MillenniumDB (RDF)

**Преимущества:** Collaborative, визуализация, стандартный RDF/OWL.  
**Недостатки:** Ручная конвертация RDF ↔ Property Graph.

---

#### Подход 3: MillenniumDB + SPARQL (Этап 3, продвинутый)

**Применение:** Формальный inference (логический вывод) на основе онтологии.

```sparql
# SPARQL inference: вывести неявные факты
PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>

SELECT ?employee WHERE {
  ?employee rdf:type :Person .
  ?employee :worksFor ?org .
  ?org rdf:type :TechCompany .
  
  # Inference: TechCompany subClassOf Organization
  # → ?org также имеет тип Organization
}
```

**Преимущества:** Формальная семантика, inference, SPARQL.  
**Недостатки:** Две БД (MillenniumDB + PostgreSQL), синхронизация.

---

### 7.4. Рекомендация для проекта ferag

**Этап 1 (текущий):** PostgreSQL + AGE как основная БД, Pydantic-схемы для онтологии (быстрый старт).

**Этап 2 (в перспективе):** Добавить MillenniumDB как источник истины, WebProtégé для онтологии, Schema Induction с MS GraphRAG.

**Этап 3 (продвинутый):** SPARQL inference, формальная валидация онтологии.

---

### 7.5. Выбор модели для Schema Induction

| Задача | Модель | Размер | Применение |
|--------|--------|--------|------------|
| **Извлечение триплетов** (MS GraphRAG) | Llama 3.3 70B Q4_K_M | 40 ГБ | Индексация корпуса (2-4 часа) |
| **Schema Induction** | Llama 3.3 70B Q4_K_M | 40 ГБ | Извлечение онтологии (2-4 часа) |
| **Диалог с пользователем** (RAG) | Llama 3.3 70B Q4_K_M или меньше | 40 ГБ / 7 ГБ | Ответы на вопросы (реал-тайм) |

**Для данной системы (64 ГБ ОЗУ):** Llama 3.3 70B Q4_K_M — оптимальный выбор для всех задач. Для live-диалога можно использовать меньшую модель (Llama 3.2 7B) для скорости.

---

## 8. PostgreSQL как multi-model основа для эксплуатации

### 8.1. Почему PostgreSQL + расширения

**Проблема ArangoDB CE (отклонена):**
- ❌ Лимит 100 GB данных
- ❌ Только некоммерческое использование
- ❌ Нет гарантий масштабирования кластера

**Решение: PostgreSQL + Apache AGE + pgvector**
- ✅ Полностью open source, без ограничений
- ✅ Нет лимита на размер данных
- ✅ Можно использовать коммерчески
- ✅ Multi-model через расширения
- ✅ Стабильность и зрелость (30+ лет)

---

### 8.2. Архитектура хранения

```
┌──────────────────────────────────────────────┐
│  MillenniumDB (Источник истины)              │
│  • RDF/SPARQL + OWL онтология                │
│  • Все триплеты в RDF формате                │
│  • Формальная семантика, inference           │
└──────────────┬───────────────────────────────┘
               ↓ синхронизация
┌──────────────────────────────────────────────┐
│  PostgreSQL 16 (Рабочая БД)                  │
│  ┌────────────────────────────────────────┐  │
│  │ Apache AGE (Property Graph)            │  │
│  │ • Cypher-запросы, быстрая навигация    │  │
│  ├────────────────────────────────────────┤  │
│  │ pgvector (векторный поиск)             │  │
│  │ • Эмбеддинги документов                │  │
│  │ • Семантический поиск для RAG          │  │
│  ├────────────────────────────────────────┤  │
│  │ Реляционные таблицы (SQL)              │  │
│  │ • Документы, метаданные                │  │
│  │ • Materialized Views (агрегация)       │  │
│  ├────────────────────────────────────────┤  │
│  │ JSONB (документы)                      │  │
│  │ • Гибкая схема для API                 │  │
│  └────────────────────────────────────────┘  │
│         Единая ACID транзакция              │
└──────────────────────────────────────────────┘
```

---

### 8.3. RAG для диалога с пользователем

**Цель:** Ответы на вопросы пользователя на основе накопленных знаний.

**Hybrid Retrieval (гибридный поиск):**

```python
# Вопрос пользователя
user_question = "Кто работает в ACME Corp?"

# 1. Векторный поиск (pgvector)
similar_docs = vector_search(user_question, top_k=5)

# 2. Граф-обход (Apache AGE Cypher)
cypher = """
MATCH (p:Person)-[:WORKS_FOR]->(o:Organization {name: 'ACME Corp'})
RETURN p.name AS employee
"""
graph_results = age_query(cypher)

# 3. Реляционная аналитика (SQL, опционально)
sql = "SELECT COUNT(*) FROM org_stats WHERE org_name = 'ACME Corp'"
stats = postgres_query(sql)

# 4. Генерация ответа (Ollama)
context = {
    "documents": similar_docs,
    "graph": graph_results,
    "stats": stats
}
answer = llm.generate(f"Контекст: {context}\nВопрос: {user_question}")
```

**Инструменты:**
- **LangChain** — Hybrid Retriever (вектор + граф)
- **LlamaIndex** — RAG пайплайны
- **Ollama** — LLM для генерации ответов

---

### 8.4. Обновление знаний из диалога

**Процесс:**

```
Диалог → Новая информация → data/raw/
         ↓
Накопление 1000+ новых фактов
         ↓
Повтор цикла: MS GraphRAG → Schema Induction → Синтез
```

**Частота запуска:**
- **MS GraphRAG:** Раз в неделю/месяц (при накоплении новых данных)
- **Schema Induction:** После каждой индексации
- **Синтез:** После Schema Induction
- **RAG (диалог):** Постоянно (реал-тайм)

---

## 9. Концепция "Двух Миров" и развитие проекта

### 8.1. Текущее состояние (Этап 1)
**Windows 11 + WSL2 (Ubuntu)** — единая система на Samsung 990 Pro

**Готово:**
- Концепция и философская основа (диалектический подход)
- Выбор железа и стека
- Настройка Windows 11 Pro (драйверы AMD, Wi‑Fi RZ616, NPU, время)
- WSL2 (Ubuntu) перенесена на диск D:
- Cursor с интеграцией WSL и Ollama
- Проект `ferag` создан в WSL
- Docker Desktop (WSL 2) установлен и проверен

**В процессе:**
- Первый контейнер графовой БД (Neo4j/FalkorDB)
- Пайплайн LightRAG с привязкой к онтологии
- Разработка базовой онтологии в Protégé

### 8.2. Целевое состояние (Этап 2, планируется)
**Dual-boot: Windows 11 + Fedora 43** — разделение функций

#### Распределение систем

| Система | Диск | Функции |
|---------|------|---------|
| **Windows 11 Pro** | Samsung 990 Pro (C:, 300 ГБ) | **"Станция управления"**:<br>- Визуализация (Neo4j Browser, Protégé, QGIS)<br>- Разработка (Cursor)<br>- Чаты с Ollama<br>- Создание онтологий |
| **Fedora 43** | Второй SSD 1-2 ТБ | **"Лаборатория вычислений"**:<br>- Тяжёлая индексация (GraphRAG)<br>- Модели 70B+ с квантованием<br>- Docker-контейнеры (Neo4j, FalkorDB)<br>- Нативная производительность |
| **Общий раздел** | Samsung 990 Pro (D:, ~1.7 ТБ) | **Единое хранилище**:<br>- Модели Ollama (D:\AI_MODELS)<br>- Проекты (D:\projects)<br>- NTFS (читается обеими ОС) |

#### Преимущества разделения

1. **Производительность**: Fedora даёт +5-10% к скорости работы LLM
2. **Стабильность**: Долгие вычисления в Linux не блокируют Windows
3. **Гибкость**: Работа с онтологией в Windows, пока идёт индексация в Fedora
4. **Память**: Fedora может использовать все 64 ГБ без конкуренции
5. **Экономия**: Модели не дублируются (читаются с D: обеими системами)

### 8.3. Перспективы развития

#### Этап 3: Усиление железа (опционально)

**eGPU через OCuLink:**
- Порт OCuLink в GMKtec NucBox K8 Plus
- Рассматриваемые видеокарты: RTX 4070 Ti / 4080 (16 ГБ VRAM)
- Ускорение индексации в 3-5 раз
- Возможность работы с моделями 120B+
- В Fedora поддержка eGPU через OCuLink часто работает "из коробки"

| Параметр | Текущее (iGPU) | С eGPU (RTX 4070 Ti) |
|----------|----------------|----------------------|
| VRAM | 8 ГБ | 16 ГБ |
| Скорость индексации | ~10-20 с/страница | ~3-5 с/страница |
| Максимальная модель | 30B (Q4) | 70B+ (Q5/Q6) |
| Параллельные задачи | 1 LLM | 2-3 LLM одновременно |

#### Этап 4: Интеграция с картографией

- Автоматическая геопривязка сущностей из текста
- Пространственные запросы по графу (QGIS)
- Визуализация связей на картах
- Временная ось исторических событий

## 9. Источники решений

Решения по целям, стеку, железу и пошаговой настройке зафиксированы в чате с ИИ-ассистентом (январь-февраль 2026):

- **Файл чата**: `26-0201_chat.txt`
- **Связь с документацией**: в `docs/JOURNAL.md` события привязаны к фрагментам этого чата; в `docs/TASKS.md` задачи согласованы с описанными в чате шагами.

---

Детальный статус задач — в `docs/TASKS.md`, хронология событий — в `docs/JOURNAL.md`.
